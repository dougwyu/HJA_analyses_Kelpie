cor.preds <- colnames(site_res)[sapply(site_res, is.numeric)]
cor.preds <- cor.preds[!cor.preds %in% c("Factor1","Factor2")]
alpha <- 4*0.95
for(i in seq_along(modList)){
mod <- modList[[i]]$mod
mod.ord <- modList[[i]]$ord
sp_res <- modList[[i]]$sp
site_res <- modList[[i]]$site
an <- modList[[i]]$summ # if missing, will be NULL
#  # get predictors and significance if done anova, not then just predictors
if(!is.null(an)){
preds <- rownames(an$coefficients)
stars <- symnum(an$coefficients[, 2],
corr = FALSE, na = FALSE,
cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
symbols = c("***", "**", "*", ".", " "))
stars <- stars[!preds == "(Intercept)"]
preds <- preds[!preds == "(Intercept)"]
modName <- paste("predictors:", paste0(preds, stars, collapse = " + "))
} else  modName <- paste0("predictors: ", as.character(fm[[i]])[3])
# chk residuals
# plot(mod)
#plot factors
p1 <- plot_factors(alpha, "pa", "", sp_res, site_res, cont_pred1 = "be10", cont_pred2 = "ht")
## correlation plot
mod.cor <- cor(mod.ord$scores, site_res[,cor.preds])
p2 <- ~t_corrplot(mod.cor, "")
# Plot over coords with factors as colour fill
p3 <- plot_xy_factor1("pa", "", site_res, cont_pred2 = ht)
p4 <- plot_xy_factor2("pa", "", site_res, cont_pred2 = ht)
# put rows together
pA <- plot_grid(p2, p1, rel_widths = c(4,5), align = "h", axis = "t")
pB <- plot_grid(p3, p4) # p4
title <- ggdraw() +
draw_label(modName, fontface = 'bold', size = 15, x = 0.0, y = 0.5, hjust = 0)
m1 <- plot_grid(pA, pB, title, nrow = 3, rel_heights = c(4,5,1))
# m1 <- plot_grid(pA, pB, nrow = 2, rel_heights = c(4,5))
# m1
ggsave(paste0("Hmsc_CD/local/plots/mod_topo3_", i, ".png"), m1, width = 300, height = 250, units = "mm")
}
source("Hmsc_CD/local/L1_read_data.r")
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- paste0("outputs_minimap2_",minimaprundate,"_",samtoolsfilter,"_",samtoolsqual,"_kelpie",kelpierundate,"_",primer,"_vsearch97")
datFile <- paste0("sample_by_species_table_", samtoolsfilter, "_minimap2_", minimaprundate, "_kelpie", kelpierundate, "_uncorr.csv")
otuenv <- read.csv(file.path(gitHub, outputidxstatstabulatefolder, datFile))
# M1S1
trap <- "M1"
period <- "S1"
otuenv <- otuenv %>%
dplyr::filter(trap == trap[[1]] & period == period[[1]])
# bring in DEM stats -
minocc <- 0 # set to high number (e.g. 20) for testing
otu.ab.csv <- otuenv %>% dplyr::select(contains("__"))
otu.ab.csv <- otu.ab.csv[ , colSums(otu.ab.csv > 0) >= minocc]
# log(FSL) correction and scale to quasiprobability
otu.qp.csv <- otu.ab.csv %>%
mutate(across(contains("__"),
~ .x /(otuenv$COISpike_sum*otuenv$lysis_ratio))) %>%
mutate(across(contains("__"), ~ log(.x + 0.001))) %>%
mutate(across(contains("__"), ~ scales::rescale(.x))) # {scales}
max(otu.qp.csv) == 1 # should be TRUE
# otu.qp.csv[1:10, 1:5]
# convert to presence/absence data
otu.pa.csv <- otu.ab.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv)) == minocc # should be TRUE
colsums(otu.pa.csv)
colLT5 <- colSums(otu.pa.csv) <5
colGT5 <- colSums(otu.pa.csv) >=5
sum(colLT5, colGT5) == ncol(otu.pa.csv)
otu.lt5 <- otu.pa.csv[, colLT5]
sum(colLT5)
sum(colGT5)
otu.gt5 <- otu.pa.csv[,colGT5]
plot(rowSums(otu.lt5),rowSums(otu.gt5),
ylab = "sp rich < 5 occurrences",
xlab = "sp rich >= 5 occurrences" , pch = 16)
cor.test(rowSums(otu.lt5),rowSums(otu.gt5), method = "spearman")
cor.test(rowSums(otu.lt5),rowSums(otu.gt5), method = "spearman")$pvalue
str(cor.test(rowSums(otu.lt5),rowSums(otu.gt5), method = "spearman"))
# correlation
corT <- cor.test(rowSums(otu.lt5),rowSums(otu.gt5), method = "spearman")
corT
paste("p =", round(corT$p.value, 2),
"rho =", round(corT$estimate),2)
corT$p.value
paste("p =", round(corT$p.value, 5),
"rho =", round(corT$estimate,2))
text(38, 70, paste("p =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3)))
text(38, 70, paste("Spearman correlation\np =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3)))
png("Hmsc_CD/local/plots/spRich_cor_minocc5.png", width = 200, height = 200, units= "mm", res = 200)
plot(rowSums(otu.lt5),rowSums(otu.gt5),
ylab = "sp rich < 5 occurrences",
xlab = "sp rich >= 5 occurrences" , pch = 16)
text(38, 70, paste("Spearman correlation\np =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3)))
dev.off()
png("Hmsc_CD/local/plots/spRich_cor_minocc5.png", width = 200, height = 200, units= "mm", res = 200)
plot(rowSums(otu.lt5),rowSums(otu.gt5),
ylab = "sp rich < 5 occurrences",
xlab = "sp rich >= 5 occurrences" , pch = 16)
text(38, 70, paste("Spearman correlation\np =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3),
"n =", ncol(otu.pa.csv)))
dev.off()
png("Hmsc_CD/local/plots/spRich_cor_minocc5.png", width = 200, height = 200, units= "mm", res = 200)
plot(rowSums(otu.lt5),rowSums(otu.gt5),
ylab = "sp rich < 5 occurrences",
xlab = "sp rich >= 5 occurrences" , pch = 16)
text(38, 70, paste("Spearman correlation\np =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3),
"n =", nrow(otu.pa.csv)))
dev.off()
plot(rowSums(otu.lt5),rowSums(otu.gt5),
ylab = "sp rich < 5 occurrences",
xlab = "sp rich >= 5 occurrences" , pch = 16)
text(38, 70, paste("Spearman correlation\np =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3),
"n =", nrow(otu.pa.csv)), adj = 0)
plot(rowSums(otu.lt5),rowSums(otu.gt5),
ylab = "sp rich < 5 occurrences",
xlab = "sp rich >= 5 occurrences" , pch = 16)
text(30, 70, paste("Spearman correlation\np =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3),
"n =", nrow(otu.pa.csv)), adj = 0)
colLT5 <- colSums(otu.pa.csv) < 5
colGT5 <- colSums(otu.pa.csv) >=5
sum(colLT5)
sum(colGT5)
otu.lt5 <- otu.pa.csv[, !colLT5]
otu.gt5 <- otu.pa.csv[, !colGT5]
sum(colGT5)
colGT5 <- colSums(otu.pa.csv) >=5
sum(colGT5) #
sum(colLT5) # 268 spp have less than 5
colLT5 <- colSums(otu.pa.csv) < 5
colGT5 <- colSums(otu.pa.csv) >=5
sum(colLT5) # 942 spp have less than 5
sum(colGT5) # 268
sum(colLT5, colGT5) == ncol(otu.pa.csv)
otu.lt5 <- otu.pa.csv[, !colLT5]
otu.gt5 <- otu.pa.csv[, !colGT5]
otu.lt5 <- otu.pa.csv[, colLT5]
otu.gt5 <- otu.pa.csv[, colGT5]
corT <- cor.test(rowSums(otu.lt5),rowSums(otu.gt5), method = "spearman")
str(corT)
png("Hmsc_CD/local/plots/spRich_cor_minocc5.png", width = 200, height = 200, units= "mm", res = 200)
plot(rowSums(otu.lt5),rowSums(otu.gt5),
xlab = "sp richness (< 5 occurrences)",
ylab = "sp richness (>= 5 occurrences)" , pch = 16)
text(30, 70, paste("Spearman correlation\np =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3),
"n =", nrow(otu.pa.csv)), adj = 0)
dev.off()
png("Hmsc_CD/local/plots/spRich_cor_minocc5.png", width = 250, height = 200, units= "mm", res = 200)
plot(rowSums(otu.lt5),rowSums(otu.gt5),
xlab = "sp richness (< 5 occurrences)",
ylab = "sp richness (>= 5 occurrences)" , pch = 16)
text(30, 70, paste("Spearman correlation\np =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3),
"n =", nrow(otu.pa.csv)), adj = 0)
dev.off()
png("Hmsc_CD/local/plots/spRich_cor_minocc5.png", width = 250, height = 200, units= "mm", res = 200)
plot(rowSums(otu.lt5),rowSums(otu.gt5),
xlab = "sp richness (< 5 occurrences)",
ylab = "sp richness (>= 5 occurrences)" , pch = 16)
text(32, 70, paste("Spearman correlation\np =", round(corT$p.value, 5),
"rho =", round(corT$estimate,3),
"n =", nrow(otu.pa.csv)), adj = 0)
dev.off()
library(glmnetUtils)
library(glmnet)
library(glmnetUtils)
library(doParallel)
library(foreach)
getwd()
wd <- here::here()
setwd(wd)
source("Hmsc_CD/oregon_ada/S1_read_data.r")
source("Hmsc_CD/oregon_ada/code/S1_read_data.r")
source('J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada/code/S1_read_data.r', echo=TRUE)
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
source('J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada/code/S1_read_data.r', echo=TRUE)
head(env.vars)
preds <- colnames(env.vars)
XFormula <- as.formula(paste0("~ ", paste0(preds, collapse = " + ")))
XFormula
X <- model.matrix(XFormula, env.vars)[,-1] # let glmnet put the intercept on...
head(X)
sum(apply(X, 2, is.na))
range(otu.pa.csv)
Y <- as.matrix(otu.pa.csv)
foldid=sample(1:10,size=nrow(Y),replace=TRUE) # keep same for using different alpha
alpha = seq(0,1,0.1)
table(foldid)
Y <- as.matrix(log(Y.train.qp+0.001))
sum(apply(Y, 2, is.nan))
sum(apply(Y, 2, is.infinite))
cva1 <- cva.glmnet(x = X, y = Y,
family = "mgaussian",
intercept = TRUE,
standardize = FALSE)
wd <- here::here()
setwd(file.path(wd, "Hmsc_CD/oregon_ada"))
dir()
wd <- here::here()
wd
setwd(file.path(wd, "Hmsc_CD/oregon_ada"))
dir()
getwd()
source('J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/local/S1_read_data_local.r', echo=TRUE)
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- glue::glue("outputs_minimap2_{minimaprundate}_{samtoolsfilter}_{samtoolsqual}_kelpie{kelpierundate}_{primer}_vsearch97")
datFile <- glue("sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_uncorr.csv")
otuenv <- read.csv(file.path(gitHub, outputidxstatstabulatefolder, datFile))
# M1S1
trap <- "M1"
period <- "S1"
otuenv <- otuenv %>%
dplyr::filter(trap == trap[[1]] & period == period[[1]])
# clean up
rm(datFile, gitHub, kelpierundate, minimaprundate, outputidxstatstabulatefolder, period, primer, samtoolsfilter, samtoolsqual, trap)
# bring in DEM stats
# load("data/demStats.rdata") # temporary location for moment...  REPLACED By topo.df
# load new topo vars
load("data/topo_data.rdata")
minocc <- 5 # set to high number (e.g. 20) for testing
otu.ab.csv <- otuenv %>% dplyr::select(contains("__"))
otu.ab.csv <- otu.ab.csv[ , colSums(otu.ab.csv > 0) >= minocc]
# log(FSL) correction and scale to quasiprobability
otu.qp.csv <- otu.ab.csv %>%
mutate(across(contains("__"),
~ .x /(otuenv$COISpike_sum*otuenv$lysis_ratio))) %>%
mutate(across(contains("__"), ~ log(.x + 0.001))) %>%
mutate(across(contains("__"), ~ scales::rescale(.x))) # {scales}
max(otu.qp.csv) == 1 # should be TRUE
otu.pa.csv <- otu.ab.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv)) == minocc # should be TRUE
Y.train.pa <- otu.pa.csv
Y.train.qp <- otu.qp.csv
rm(minocc)
# env covariates
env.vars <- otuenv %>%
dplyr::select(!contains("__"), -UTM_E, -UTM_N, -starts_with("nor")) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"),
elevation_m = elevation_m * 0.3048, ## convert to metres???
B1_median = apply(across(starts_with("B1_")), 1, median),
B2_median = apply(across(starts_with("B2_")), 1, median),
B3_median = apply(across(starts_with("B3_")), 1, median),
B4_median = apply(across(starts_with("B4_")), 1, median),
B5_median = apply(across(starts_with("B5_")), 1, median),
B6_median = apply(across(starts_with("B6_")), 1, median),
B7_median = apply(across(starts_with("B7_")), 1, median),
B10_median = apply(across(starts_with("B10_")), 1, median),
B11_median = apply(across(starts_with("B11_")), 1, median),
lg_DistStream = log(distToStream_m + 0.001),
lg_DistRoad = log(distToRoad_m + 0.001),
lg_YrsDisturb = log(YrsSinceDist + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001)) %>%
dplyr::select(uniqueID, clearcut,insideHJA,oldGrowthIndex, elevation_m, canopyHeight_m, precipitation_mm, minT_annual, maxT_annual, mean.NDVI, mean.EVI, mean.green, mean.wet, mean.bright, l_p25, l_rumple, B1_median, B2_median,B3_median,B4_median,B5_median,B6_median,B7_median,B10_median,B11_median,lg_DistStream, lg_DistRoad, lg_YrsDisturb, lg_cover2m_max, lg_cover2m_4m, lg_cover4m_16m) %>%
dplyr::left_join(y = topo.df, by = "uniqueID") %>%
mutate(across(where(is.numeric), scale), # scale here
clearcut = factor(clearcut),
insideHJA = factor(insideHJA)) %>%
env.vars <- otuenv %>%
dplyr::select(!contains("__"), -UTM_E, -UTM_N, -starts_with("nor")) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"),
elevation_m = elevation_m * 0.3048, ## convert to metres???
B1_median = apply(across(starts_with("B1_")), 1, median),
B2_median = apply(across(starts_with("B2_")), 1, median),
B3_median = apply(across(starts_with("B3_")), 1, median),
B4_median = apply(across(starts_with("B4_")), 1, median),
B5_median = apply(across(starts_with("B5_")), 1, median),
B6_median = apply(across(starts_with("B6_")), 1, median),
B7_median = apply(across(starts_with("B7_")), 1, median),
B10_median = apply(across(starts_with("B10_")), 1, median),
B11_median = apply(across(starts_with("B11_")), 1, median),
lg_DistStream = log(distToStream_m + 0.001),
lg_DistRoad = log(distToRoad_m + 0.001),
lg_YrsDisturb = log(YrsSinceDist + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001)) %>%
dplyr::select(uniqueID, clearcut,insideHJA,oldGrowthIndex, elevation_m, canopyHeight_m, precipitation_mm, minT_annual, maxT_annual, mean.NDVI, mean.EVI, mean.green, mean.wet, mean.bright, l_p25, l_rumple, B1_median, B2_median,B3_median,B4_median,B5_median,B6_median,B7_median,B10_median,B11_median,lg_DistStream, lg_DistRoad, lg_YrsDisturb, lg_cover2m_max, lg_cover2m_4m, lg_cover4m_16m) %>%
dplyr::left_join(y = topo.df, by = "uniqueID") %>%
mutate(across(where(is.numeric), scale), # scale here
clearcut = factor(clearcut),
insideHJA = factor(insideHJA))
wd <- here::here()
wd
setwd(wd)
dir()
getwd()
library(dplyr)
# library(here) # not on ADA
library(glue)
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- glue::glue("outputs_minimap2_{minimaprundate}_{samtoolsfilter}_{samtoolsqual}_kelpie{kelpierundate}_{primer}_vsearch97")
datFile <- glue("sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_uncorr.csv")
otuenv <- read.csv(file.path(gitHub, outputidxstatstabulatefolder, datFile))
# M1S1
trap <- "M1"
period <- "S1"
otuenv <- otuenv %>%
dplyr::filter(trap == trap[[1]] & period == period[[1]])
# clean up
rm(datFile, gitHub, kelpierundate, minimaprundate, outputidxstatstabulatefolder, period, primer, samtoolsfilter, samtoolsqual, trap)
# bring in DEM stats
# load("data/demStats.rdata") # temporary location for moment...  REPLACED By topo.df
# load new topo vars
load("Hmsc_CD/oregon_ada/data/topo_data.rdata")
minocc <- 5 # set to high number (e.g. 20) for testing
otu.ab.csv <- otuenv %>% dplyr::select(contains("__"))
otu.ab.csv <- otu.ab.csv[ , colSums(otu.ab.csv > 0) >= minocc]
# log(FSL) correction and scale to quasiprobability
otu.qp.csv <- otu.ab.csv %>%
mutate(across(contains("__"),
~ .x /(otuenv$COISpike_sum*otuenv$lysis_ratio))) %>%
mutate(across(contains("__"), ~ log(.x + 0.001))) %>%
mutate(across(contains("__"), ~ scales::rescale(.x))) # {scales}
max(otu.qp.csv) == 1 # should be TRUE
# otu.qp.csv[1:10, 1:5]
# convert to presence/absence data
otu.pa.csv <- otu.ab.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv)) == minocc # should be TRUE
Y.train.pa <- otu.pa.csv
Y.train.qp <- otu.qp.csv
rm(minocc)
env.vars <- otuenv %>%
dplyr::select(!contains("__"), -UTM_E, -UTM_N, -starts_with("nor")) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"),
elevation_m = elevation_m * 0.3048, ## convert to metres???
B1_median = apply(across(starts_with("B1_")), 1, median),
B2_median = apply(across(starts_with("B2_")), 1, median),
B3_median = apply(across(starts_with("B3_")), 1, median),
B4_median = apply(across(starts_with("B4_")), 1, median),
B5_median = apply(across(starts_with("B5_")), 1, median),
B6_median = apply(across(starts_with("B6_")), 1, median),
B7_median = apply(across(starts_with("B7_")), 1, median),
B10_median = apply(across(starts_with("B10_")), 1, median),
B11_median = apply(across(starts_with("B11_")), 1, median),
lg_DistStream = log(distToStream_m + 0.001),
lg_DistRoad = log(distToRoad_m + 0.001),
lg_YrsDisturb = log(YrsSinceDist + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001)) %>%
dplyr::select(uniqueID, clearcut,insideHJA,oldGrowthIndex, elevation_m, canopyHeight_m, precipitation_mm, minT_annual, maxT_annual, mean.NDVI, mean.EVI, mean.green, mean.wet, mean.bright, l_p25, l_rumple, B1_median, B2_median,B3_median,B4_median,B5_median,B6_median,B7_median,B10_median,B11_median,lg_DistStream, lg_DistRoad, lg_YrsDisturb, lg_cover2m_max, lg_cover2m_4m, lg_cover4m_16m) %>%
dplyr::left_join(y = topo.df, by = "uniqueID") %>%
mutate(across(where(is.numeric), scale), # scale here
clearcut = factor(clearcut),
insideHJA = factor(insideHJA))
# variable selection made a S2_define_models
X.train <- env.vars
## Study design data
S.train <- otuenv %>%
dplyr::select(SiteName,trap,period, UTM_E, UTM_N) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"))
head(S.train)
spp <- data.frame(species = colnames(Y.train.pa)) %>%
tidyr::separate(col = species, into = c("OTU", "empty", "class", "order", "family",
"genus", "epithet", "BOLD", "BOLDID",
"size"),
remove = FALSE, sep = "_") %>%
select(-empty)
head(spp)
# convert to NAs
for(c in seq_along(spp)[-1]) spp[,c] <- sub("NA", NA, spp[,c])
# Add dummy family and genus
spp$family[is.na(spp$family)] <- sprintf("fam%03d", 1:sum((is.na(spp$family))))
spp$genus[is.na(spp$genus)] <- sprintf("gen%03d", 1:sum((is.na(spp$genus))))
head(spp)
# convert to factors for ape
spp <- spp[order(spp$class, spp$order, spp$family, spp$genus),]
tax.cols <- c("class", "order", "family", "genus", "epithet", "species")
for(i in tax.cols) spp[,i] <- factor(spp[,i])
head(spp)
P <- ape::as.phylo(~class/order/family/genus/species, data = spp, collapse = F)
P$edge.length = rep(1, length(P$edge)) # make all lengths eqaul between tax levels
ape::is.rooted(P)
all(P$tip.label %in% colnames(Y.train.pa))
all(P$tip.label %in% colnames(Y.train.qp))
# save(Y.train.pa, Y.train.qp, X.train, S.train, P, file = "data/allData_vif.rdata")
rm(c, i, tax.cols, spp)
write.csv(env.vars, "Hsmc_CD/local/data/biodiversity_site_info_GIS_vars_20210115.csv", row.names = F)
write.csv(env.vars, "Hmsc_CD/local/data/biodiversity_site_info_GIS_vars_20210115.csv", row.names = F)
write.csv(env.vars, "HJA_scripts/10_eo_data/biodiversity_site_info_GIS_vars_20210115.csv", row.names = F)
devtools::install_github("https://github.com/TheoreticalEcology/s-jSDM", subdir = "sjSDM")
install.packages(c("glmnet", "htmltools", "MCMCpack", "pROC", "RcppArmadillo"))
devtools::install_github("https://github.com/TheoreticalEcology/s-jSDM", subdir = "sjSDM")
library(sJSDM)
library(sjSDM)
n
pack <- "sjSDM"
path <- find.package(pack)
system(paste(shQuote(file.path(R.home("bin"), "R")),"CMD", "Rd2pdf", shQuote(path)))
file.path(R.home("bin"), "R")
shQuote(file.path(R.home("bin"), "R")),"CMD", "Rd2pdf", shQuote(path)
shQuote(file.path(R.home("bin"), "R")),"CMD", "Rd2pdf", shQuote(path))
shQuote(file.path(R.home("bin"), "R")),"CMD", "Rd2pdf", shQuote(path)
shQuote(file.path(R.home("bin"), "R")),"CMD", "Rd2pdf", shQuote(path))
paste(shQuote(file.path(R.home("bin"), "R")),"CMD", "Rd2pdf", shQuote(path))
devtools::build_manual(path)
set.seed(42)
community <- simulate_SDM(sites = 100, species = 10, env = 3)
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
source('J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada/code_sjSDM/S1_read_data.r', echo=TRUE)
Y <- as.matrix(otu.pa.csv)
raretaxa <- which(colSums(Y.train.pa > 0) < 10)
length(raretaxa)
Y.train.pa2 <- Y.train.pa[, -raretaxa] # reduced species
rm(raretaxa)
Y.train.pa_min10 <- Y.train.pa[, -raretaxa] # reduced species
raretaxa <- which(colSums(Y.train.pa > 0) < 10)
length(raretaxa)
Y.train.pa_min10 <- Y.train.pa[, -raretaxa] # reduced species
rm(raretaxa)
Y.train.pa_min10 <- as.matrix(Y.train.pa[, -raretaxa]) # reduced species
raretaxa <- which(colSums(Y.train.pa > 0) < 10)
length(raretaxa)
Y.train.pa_min10 <- as.matrix(Y.train.pa[, -raretaxa]) # reduced species
rm(raretaxa)
all(all.vars(XFormula1) %in% names(X.train))
XFormula1 <- as.formula(~be10+B11_median+mean.EVI+insideHJA + Ess + ht + ht.r500 + cov4_16 + cov4_16.r500 + mTopo)
all(all.vars(XFormula1) %in% names(X.train))
mm <- model.matrix(XFormula1, data = env.vars)
head(mm)
head(S.train)
load("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada/code_sjSDM/sjSDM_trial/sjDM_trial_mods.rdata")
head(mm)
typeof(XFormula1)
str(X.train)
head(as.matrix(X.train))
form1 <- as.formula(~V1+V2+V3)
form1
class(form1)
form1 <- formula(~V1+V2+V3)
form1
class(form1)
match.call(form1)
mf <- match.call()
mf
match("formula", names(mf))
formula <- as.formula(~V1+V2+V3)
mf <- match.call()
match("formula", names(mf))
names(mf)
linear2 = function(data = NULL, formula = NULL, lambda = 0.0, alpha = 0.5) {
if(is.data.frame(data)) {
if(!is.null(formula)){
mf = match.call()
m = match("formula", names(mf))
formula = stats::as.formula(mf[m]$formula)
X = stats::model.matrix(formula, data)
} else {
formula = stats::as.formula("~.")
X = stats::model.matrix(formula, data)
}
form1 <- as.formula(~V1+V2+V3)
form1
linear2(data = Env, formula = form1)
community <- simulate_SDM(sites = 100, species = 10, env = 3)
form1 <- as.formula(~V1+V2+V3)
form1
Env <- data.frame(rnorm(25),rnorm(25),rnorm(25))
head(Env)
Env <- data.frame(V1 = rnorm(25), V2 = rnorm(25),V3 =rnorm(25))
head(Env)
linear2(data = Env, formula = form1)
mf = match.call()
formula = form1
if(!is.null(formula)){
mf = match.call()
m = match("formula", names(mf))
formula = stats::as.formula(mf[m]$formula)
X = stats::model.matrix(formula, data)
}
!is.null(formula)
data = Env
if(!is.null(formula)){
mf = match.call()
m = match("formula", names(mf))
formula = stats::as.formula(mf[m]$formula)
X = stats::model.matrix(formula, data)
}
!is.null(formula)
mf = match.call()
m = match("formula", names(mf))
formula = stats::as.formula(mf[m]$formula)
X = stats::model.matrix(formula, data)
community <- sjSDM::simulate_SDM(sites = 100, species = 10, env = 3)
