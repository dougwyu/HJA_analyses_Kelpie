"_meanEVAL_",
abund,
"_min",
minocc,
"_nSteps",
noSteps,
".csv")))
head(res)
res.best <- res[which.max(res$AUC.test_mean),,drop = T]
res.best
rm(res)
# Choose pa or qp reponse data and family
if(abund == "pa") {
Y <- otu.pa.csv
family <- stats::binomial('probit') } else {
if(abund ==  "qp") {
Y <- otu.qp.csv
family <- stats::binomial('probit') # check other family?
} else stop("check abund")
}
scale.env <- allVars.sc %>%
filter(SiteName %in% env.vars$SiteName) %>% ## env.vars is the validation set (75% - select.percent)
dplyr::select(all_of(vars))
library(dplyr)
scale.env <- allVars.sc %>%
filter(SiteName %in% env.vars$SiteName) %>% ## env.vars is the validation set (75% - select.percent)
dplyr::select(all_of(vars))
View(allVars.sc)
newData.sc <- newData.sc %>%
dplyr::select(all_of(vars))
scale.XY <- xy.sites.sc %>%
filter(SiteName %in% env.vars$SiteName) %>%
dplyr::select(c("UTM_E", "UTM_N"))%>%
as.matrix()
gis_in <- "D:/CD/UEA/Oregon/gis/raw_gis_data"
gis_out <- "D:/CD/UEA/Oregon/gis/processed_gis_data"
## Load sample points
## Load sample site points
load(file.path(gis_out, "sample_sites.rdata"))
xy.utm
## Load new version of vars
load("data/envVars.rdata")
scale.env <- allVars.sc %>%
filter(SiteName %in% train.Names) %>% ## env.vars is the validation set (75% - select.percent)
dplyr::select(all_of(vars))
resFolder <-"code_sjSDM/r20210627a/results"
if(!dir.exists(resFolder)) dir.create(resFolder, recursive = TRUE)
source("data/vif_zuur.r")
## Updated to new vars, also changes to elevation_m, canopy_height_m  to _f.
# # model settings:
abund <- "pa"
spChoose <- "M1S1_v16"
device <- "gpu"
iter <- 170L
sampling <- 5000L
## Number of samples from tuning grid - random search
noSteps <- 1000
# no of CV folds
k <- 5
# timings...
# models take approx to 0.28 mins run (Run time 00:36:11 for 125 models)
# noSteps * k * 0.28/60
noSteps * k
# Storage
# About ~600k per model .rds -- in GB
# noSteps * k * 600  / 1048576
# for testing on cpu
# device <- "cpu"
# iter <- 10L
# sampling <- 100L
# noSteps <- 2
# k <- 2
### 1. Get data from github #####
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- paste0("outputs_minimap2_",
minimaprundate,"_",
samtoolsfilter,"_",
samtoolsqual,
"_kelpie",
kelpierundate,
"_",
primer,
"_vsearch97")
datFile <- paste0("sample_by_species_table_",
samtoolsfilter,
"_minimap2_",
minimaprundate,
"_kelpie",
kelpierundate,
"_FSL_qp.csv")
# file path:
fn <- file.path(gitHub, outputidxstatstabulatefolder, datFile)
# what file am i using?
basename(fn)
# when was it modified? - only if stored locally.
file.mtime(fn)
# read complete data set
otuenv <- read.csv(fn, stringsAsFactors = FALSE, na.strings = "NA")
library(dplyr)
resFolder <-"code_sjSDM/r20210627d/results"
if(!dir.exists(resFolder)) dir.create(resFolder, recursive = TRUE)
set.seed(99)
source("data/vif_zuur.r")
## Updated to new vars, also changes to elevation_m, canopy_height_m  to _f.
# # model settings:
abund <- "pa"
spChoose <- "M1S1_v16"
device <- "gpu"
iter <- 170L
sampling <- 5000L
## Number of samples from tuning grid - random search
noSteps <- 1000
# no of CV folds
k <- 5
# timings...
# models take approx to 0.28 mins run (Run time 00:36:11 for 125 models)
# noSteps * k * 0.28/60
noSteps * k
# Storage
# About ~600k per model .rds -- in GB
# noSteps * k * 600  / 1048576
# for testing on cpu
# device <- "cpu"
# iter <- 10L
# sampling <- 100L
# noSteps <- 2
# k <- 2
### 1. Get data from github #####
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- paste0("outputs_minimap2_",
minimaprundate,"_",
samtoolsfilter,"_",
samtoolsqual,
"_kelpie",
kelpierundate,
"_",
primer,
"_vsearch97")
datFile <- paste0("sample_by_species_table_",
samtoolsfilter,
"_minimap2_",
minimaprundate,
"_kelpie",
kelpierundate,
"_FSL_qp.csv")
# file path:
fn <- file.path(gitHub, outputidxstatstabulatefolder, datFile)
# what file am i using?
basename(fn)
# when was it modified? - only if stored locally.
file.mtime(fn)
# read complete data set
otuenv <- read.csv(fn, stringsAsFactors = FALSE, na.strings = "NA")
# keep OTUs with >= minocc incidences AND with presnece at both M1 or M2
minocc <- 25 # set to high number (e.g. 20) for testing
### CHOOSE species by minocc, taxon,
## get Species columns by M1 and M2, with minocc calculated per trap
## can choose below whether to include just species shared between M1 and M2
spM <- otuenv %>%
dplyr::filter(period == "S1") %>%
dplyr::select(SiteName, trap, contains("__")) %>%
tidyr::pivot_longer(cols = contains("__"), names_to = "OTU", values_drop_na = FALSE) %>%
mutate(value = value>0) %>% # change to PA
group_by(OTU, trap) %>%
summarise(nSites = sum(value, na.rm = T)) %>% # Number of sites at which present
filter(nSites >= minocc) %>% # filter by minocc
ungroup() %>%
tidyr::pivot_wider(names_from = trap, values_from = nSites, values_fn = function(x) sum(x)>0) %>%
#filter(M1) %>% # CHOOOSE HERE FOR SINGLE. OR SHARED TRAP SPECIES GFROUP: filter(M1 & M2)
tidyr::separate(col = OTU, into = c("ID", "empty", "class", "order", "family",
"genus", "epithet", "BOLD", "BOLDID", "size"),
remove = FALSE, sep = "_") %>%
#dplyr::filter(order == "Diptera")%>%
dplyr::select(OTU)
nrow(spM)
getwd()
minocc <- 20 # set to high number (e.g. 20) for testing
### CHOOSE species by minocc, taxon,
## get Species columns by M1 and M2, with minocc calculated per trap
## can choose below whether to include just species shared between M1 and M2
spM <- otuenv %>%
dplyr::filter(period == "S1") %>%
dplyr::select(SiteName, trap, contains("__")) %>%
tidyr::pivot_longer(cols = contains("__"), names_to = "OTU", values_drop_na = FALSE) %>%
mutate(value = value>0) %>% # change to PA
group_by(OTU, trap) %>%
summarise(nSites = sum(value, na.rm = T)) %>% # Number of sites at which present
filter(nSites >= minocc) %>% # filter by minocc
ungroup() %>%
tidyr::pivot_wider(names_from = trap, values_from = nSites, values_fn = function(x) sum(x)>0) %>%
#filter(M1) %>% # CHOOOSE HERE FOR SINGLE. OR SHARED TRAP SPECIES GFROUP: filter(M1 & M2)
tidyr::separate(col = OTU, into = c("ID", "empty", "class", "order", "family",
"genus", "epithet", "BOLD", "BOLDID", "size"),
remove = FALSE, sep = "_") %>%
#dplyr::filter(order == "Diptera")%>%
dplyr::select(OTU)
nrow(spM)
# keep OTUs with >= minocc incidences AND with presnece at both M1 or M2
minocc <- 20 # set to high number (e.g. 20) for testing
### CHOOSE species by minocc, taxon,
## get Species columns by M1 and M2, with minocc calculated per trap
## can choose below whether to include just species shared between M1 and M2
spM <- otuenv %>%
dplyr::filter(period == "S1") %>%
dplyr::select(SiteName, trap, contains("__")) %>%
tidyr::pivot_longer(cols = contains("__"), names_to = "OTU", values_drop_na = FALSE) %>%
mutate(value = value>0) %>% # change to PA
group_by(OTU, trap) %>%
summarise(nSites = sum(value, na.rm = T)) %>% # Number of sites at which present
filter(nSites >= minocc) %>% # filter by minocc
ungroup() %>%
tidyr::pivot_wider(names_from = trap, values_from = nSites, values_fn = function(x) sum(x)>0) %>%
#filter(M1) %>% # CHOOOSE HERE FOR SINGLE. OR SHARED TRAP SPECIES GFROUP: filter(M1 & M2)
tidyr::separate(col = OTU, into = c("ID", "empty", "class", "order", "family",
"genus", "epithet", "BOLD", "BOLDID", "size"),
remove = FALSE, sep = "_") %>%
#dplyr::filter(order == "Diptera")%>%
dplyr::select(OTU)
nrow(spM)
minocc <- 20 # set to high number (e.g. 20) for testing
### CHOOSE species by minocc, taxon,
## get Species columns by M1 and M2, with minocc calculated per trap
## can choose below whether to include just species shared between M1 and M2
spM <- otuenv %>%
dplyr::filter(period == "S1") %>%
dplyr::select(SiteName, trap, contains("__")) %>%
tidyr::pivot_longer(cols = contains("__"), names_to = "OTU", values_drop_na = FALSE) %>%
mutate(value = value>0) %>% # change to PA
group_by(OTU, trap) %>%
summarise(nSites = sum(value, na.rm = T)) %>% # Number of sites at which present
filter(nSites >= minocc) %>% # filter by minocc
ungroup() %>%
tidyr::pivot_wider(names_from = trap, values_from = nSites, values_fn = function(x) sum(x)>0) %>%
#filter(M1) %>% # CHOOOSE HERE FOR SINGLE. OR SHARED TRAP SPECIES GFROUP: filter(M1 & M2)
tidyr::separate(col = OTU, into = c("ID", "empty", "class", "order", "family",
"genus", "epithet", "BOLD", "BOLDID", "size"),
remove = FALSE, sep = "_") %>%
#dplyr::filter(order == "Diptera")%>%
dplyr::select(OTU)
nrow(spM)
#### YL code --- Choose training and test data  ###
# ..... data belong to same sample are assigned to either train/test
# ... 75% of data as training ...
# random selection
alldata1 = otuenv %>%
filter(period == "S1") %>% #& trap == trap[1]
select(c(UTM_N, UTM_E, SiteName, trap))
head(alldata1)
dim(alldata1)
unique(alldata1$period)
num.point = nrow(alldata1)
num.sample = length(unique(alldata1$SiteName))
ran.tt = data.frame(table(alldata1$SiteName) )
colnames(ran.tt) = c('SiteName', 'NumTrap')
select.percent = .75
sel = 0
num.train = round(num.point*select.percent)
num.test = num.point - num.train
while ( sel != num.test ) {
x1 = (num.train-table(ran.tt$NumTrap)[[1]])/2
x2 = num.train - table(ran.tt$NumTrap)[[2]]*2
table(ran.tt$NumTrap)
num1 = base::sample(x1:x2, 1)
Sseq = base::sample(ran.tt$SiteName, num1)
# sample.test
sel = sum(ran.tt$NumTrap[ran.tt$SiteName %in% Sseq])
}
Sseq
# add train/test assignment
alldata1$assign = "train"
alldata1$assign[ alldata1$SiteName %in% Sseq ] = 'test'
# plot(alldata1[,c("UTM_N", "UTM_E")], asp = 1, col = as.factor(alldata1$assign), pch  =16)
head(alldata1)
tab <- table(alldata1[, c("SiteName", "assign")])
colSums(tab)
colSums(tab > 0)
train.Names <- alldata1$SiteName[alldata1$assign == "train"]
test.Names <- alldata1$SiteName[alldata1$assign == "test"]
rm(alldata1, tab, num.point, num.sample,Sseq, sel, x1, x2, ran.tt,num.train, num.test, num1)
#### #####
###  filter species here to those in sp.M1m2$OTU - already filtered for minocc
## training / validation data set
otu.qp.csv <- otuenv %>%
dplyr::filter(period == "S1" & SiteName %in% train.Names) %>% ## filter for sites in trainig/validation data
dplyr::select(spM$OTU) ## species filter for minocc or taxon
# convert to presence/absence data
otu.pa.csv <- otu.qp.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv))  # should be TRUE - not if we filter on minocc on whole data set...
### do testing data set:
otu.qp.csv.test <- otuenv %>%
dplyr::filter(period == "S1" & SiteName %in% test.Names) %>% ## filter for sites in trainig/validation data
dplyr::select(spM$OTU) ## species filter for minocc or taxon
# convert to presence/absence data
otu.pa.csv.test <- otu.qp.csv.test
otu.pa.csv.test[otu.pa.csv.test > 0] <- 1
min(colSums(otu.pa.csv.test)) # - not if we filter on minocc on whole data set...
# clean up
rm(datFile, gitHub, kelpierundate, minimaprundate, outputidxstatstabulatefolder, primer, samtoolsfilter, samtoolsqual, fn, spM)
# remove OTUs, XY, and normalised NDVI and EVI
# average, optionally log, select, and scale env covariates
## Load new version of vars
load("data/envVars.rdata")
# head(allVars)
env.vars <- otuenv %>%
dplyr::filter(period == "S1") %>%
dplyr::select(trap, period, UTM_E, UTM_N, SiteName) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_")) %>%
left_join(y = allVars, by = "SiteName") %>%
mutate(lg_DistStream = log(DistStream + 0.001),
lg_DistRoad = log(DistRoad + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001))
# str(env.vars)
# head(env.vars)
# cat(paste(colnames(env.vars), collapse = '", "'))
# "insideHJA","cut_msk", "cut_40msk",
all.vars <- c("ht30", "gt4_r30", "gt4_250", "gt4_500", "cut_r1k", "cut_r500", "cut_r250", "cut40_r1k", "cut40_r500",
"cut40_r250", "be30", "tri30","slope30", "Nss30", "Ess30", "twi30", "tpi250", "tpi500", "tpi1k", "l_p25", "l_p95",
"l_rumple", "ndmi_stdDev_r100","ndmi_stdDev_r250", "ndmi_stdDev_r500", "nbr_stdDev_r100", "nbr_stdDev_r250",
"nbr_stdDev_r500", "ndvi_p5_r100", "ndvi_p5_r250","ndvi_p5_r500", "ndvi_p50_r100", "ndvi_p50_r250", "ndvi_p50_r500",
"ndvi_p95_r100", "ndvi_p95_r250", "ndvi_p95_r500", "ndmi_p5_r100","ndmi_p5_r250", "ndmi_p5_r500", "ndmi_p50_r100",
"ndmi_p50_r250", "ndmi_p50_r500", "ndmi_p95_r100", "ndmi_p95_r250", "ndmi_p95_r500","LC08_045029_20180726_B1",
"LC08_045029_20180726_B3", "LC08_045029_20180726_B4", "LC08_045029_20180726_B5", "LC08_045029_20180726_B7",
"LC08_045029_20180726_B10", "lg_DistStream", "lg_DistRoad", "lg_cover2m_max", "lg_cover2m_4m", "lg_cover4m_16m")
sum(!complete.cases(env.vars[,all.vars]))
head(env.vars[,all.vars])
## YL code for vif ####
dd = env.vars[,all.vars]
varrem = 'a'; maxvif = 100
names(dd)
while  (maxvif>=8 ) {
if (varrem!='a') { dd = select(dd, -all_of(varrem)) }
vif = corvif(dd)
order = order(vif$GVIF, decreasing=F)
vif.count = data.frame(var=names(dd)[order], vif=vif[order,])
maxvif = max(vif.count$vif)
varrem = as.character(vif.count$var[ncol(dd)])
print(c(ncol(dd), varrem))
}
vars <- c(vif.count$var, "insideHJA")
varsName <- "vars9"
rm(dd, varrem, vif, order)
## separate env.vars into evaluation and non evaluation data sets
env.vars.test <- env.vars[env.vars$SiteName %in% test.Names, ]
env.vars <- env.vars[env.vars$SiteName %in% train.Names, ]
#####
# old vars
# oldVars <- c("insideHJA", "elevation_f", "canopyHeight_f", "minT_annual", "precipitation_mm", "distToRoad_m", "distToStream_m", "YrsSinceDist", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_Cover_2m_max", "l_Cover_2m_4m", "l_Cover_4m_16m", "l_p25", "l_p95", "l_rumple")
#
# # new vars
# newvars <- c("be10", "tri", "slope", "Nss", "Ess", "ht", "ht.r250", "ht.r500", "ht.r1k", "cov2_4", "cov2_4.r250", "cov2_4.r500", "cov2_4.r1k", "cov4_16", "cov4_16.r250", "cov4_16.r500", "cov4_16.r1k", "be500", "mTopo", "cut.r1k.pt", "insideHJA", "minT_annual", "maxT_annual", "precipitation_mm", "lg_DistStream", "lg_DistRoad", "lg_YrsDisturb", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_p25", "l_rumple")
# varsName <- "vars3"
# vars <- c("be10", "slope", "tri", "Nss", "Ess","ndmi_stdDev", "ndvi_p5", "ndvi_p50", "ndvi_p95", "ndmi_p5", "ndmi_p50", "ndmi_p95", "savi_p50", "LC08_045029_20180726_B1", "LC08_045029_20180726_B3", "LC08_045029_20180726_B4", "LC08_045029_20180726_B5", "LC08_045029_20180726_B7", "LC08_045029_20180726_B10", "ndmi_stdDev_100m", "ndvi_p5_100m", "ndvi_p50_100m", "ndvi_p95_100m", "ndmi_p5_100m", "ndmi_p50_100m", "ndmi_p95_100m", "savi_p50_100m", "LC08_045029_20180726_B1_100m", "LC08_045029_20180726_B3_100m", "LC08_045029_20180726_B4_100m", "LC08_045029_20180726_B5_100m", "LC08_045029_20180726_B7_100m", "LC08_045029_20180726_B10_100m", "tpi250",  "tpi500", "tpi1k" , "ht", "ht.r250", "ht.r1k", "cov2_4.r250", "cov2_4.r1k", "cov4_16", "cov4_16.r250", "cov4_16.r1k", "mTopo", "cut.r1k.pt", "insideHJA", "lg_DistStream", "lg_DistRoad", "lg_YrsDisturb", "l_p25", "l_rumple")
# varsName <- "vars5"
# vars <- c("ht30", "gt4_r30", "gt4_500", "cut_r500", "cut_r250", "cut40_r500", "cut40_r250", "be30", "tri30", "slope30", "Nss30", "Ess30", "twi30", "tpi250", "tpi500", "tpi1k", "l_rumple", "insideHJA", "ndmi_stdDev_r100", "nbr_stdDev_r100", "ndvi_p5_r100", "ndvi_p50_r100", "ndvi_p95_r100", "ndmi_p5_r100", "ndmi_p50_r100", "ndmi_p95_r100", "LC08_045029_20180726_B1", "LC08_045029_20180726_B3", "LC08_045029_20180726_B4", "LC08_045029_20180726_B5", "LC08_045029_20180726_B7", "LC08_045029_20180726_B10", "minT_annual", "precipitation_mm", "lg_DistStream", "lg_DistRoad", "lg_cover2m_max", "lg_cover2m_4m", "lg_cover4m_16m")
# varsName <- "vars6"
# vars <- c("ht30","gt4_r30","gt4_250",
#           "cut_r250","cut40_r1k","cut40_r250",
#           "be30", "slope30", "Nss30","Ess30","twi30","tpi250","tpi1k",
#           "l_Cover_2m_max", "l_Cover_4m_16m", "l_rumple",
#           "DistStream","DistRoad",
#           "ndmi_stdDev_r100", "ndmi_stdDev_r500","nbr_stdDev_r100", "nbr_stdDev_r500", "ndvi_p5_r100","ndvi_p5_r500",
#           "ndvi_p50_r100","ndvi_p50_r500", "ndmi_p50_r100", "ndmi_p50_r500",
#           "LC08_045029_20180726_B4","LC08_045029_20180726_B5","LC08_045029_20180726_B10",
#           "minT_annual","precipitation_mm")
# load VIF vars and hard code below
# load(file.path(resFolder, "selected-vars.rdata"))
# cat(paste(vif.count$var, collapse = '", "'))
# varsName <- "vars7" # VIF5
# vars7 <- c("gt4_r30", "gt4_500", "cut_40msk", "cut_r1k", "cut_r250", "cut40_r1k", "cut40_r250", "tri30", "Nss30", "Ess30", "twi30", "tpi250", "tpi1k", "l_Cover_2m_4m", "l_Cover_4m_16m", "l_p25", "l_rumple", "DistStream", "DistRoad", "insideHJA", "ndmi_stdDev_r100", "nbr_stdDev_r250", "ndvi_p5_r100", "ndvi_p95_r500", "ndmi_p95_r100", "ndmi_p95_r500", "LC08_045029_20180726_B3", "LC08_045029_20180726_B5", "LC08_045029_20180726_B10", "minT_annual")
# varsName <- "vars8" # be30 replaces minT
# vars <- c("Ess30", "DistRoad", "Nss30", "DistStream", "tri30", "LC08_045029_20180726_B5", "l_rumple", "cut_40msk", "insideHJA", "cut_msk", "cut40_r1k", "l_Cover_2m_4m", "l_Cover_4m_16m", "cut_r1k", "be30", "ndmi_p95_r100", "ndvi_p50_r500", "cut40_r250", "nbr_stdDev_r250", "precipitation_mm", "gt4_250", "tpi250", "cut_r250", "LC08_045029_20180726_B1", "ndvi_p50_r100", "gt4_r30", "twi30", "ndvi_p5_r100", "tpi1k", "ndvi_p5_r500", "l_p25", "LC08_045029_20180726_B10")
#
# setdiff(vars7, vars)
# intersect(vars7, vars)
#
# check names
all(vars %in% colnames(env.vars))
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie")
setwd("D:/CD/UEA/gitHRepos/HJA_analyses_Kelpie")
library(raster)
library(sf)
library(dplyr)
setwd("D:/CD/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
library(dplyr)
resFolder <-"code_sjSDM/r20210627a/results"
abund <- "pa"
dir(resFolder)
## load model data
load(file.path(resFolder, paste0("modelData_",abund,".rdata")))
head(env.vars)
## Load new data for prediction and new scaled data
load("data/newData_scaled.rdata")
load("D:/CD/UEA/Oregon/gis/processed_gis_data/r_oversize/newData_scaled.rdata")
## update device - can't load new data onto GPU - too large?? to many copies??
device <- "cpu"
# formula.env = 'envDNN'
hidden <- list(c(50L,50L,10L), c(25L,25L,10L))
## get best tune run
res <- read.csv(file.path(resFolder,paste0("manual_tuning_sjsdm_", varsName, "_", k, "CV_", spChoose,
"_meanEVAL_",
abund,
"_min",
minocc,
"_nSteps",
noSteps,
".csv")))
head(res)
res.best <- res[which.max(res$AUC.test_mean),,drop = T]
res.best
rm(res)
# Choose pa or qp reponse data and family
if(abund == "pa") {
Y <- otu.pa.csv
family <- stats::binomial('probit') } else {
if(abund ==  "qp") {
Y <- otu.qp.csv
family <- stats::binomial('probit') # check other family?
} else stop("check abund")
}
# select X data - form globally scaled data (with newData), no validation - this is final best model
## Also filter S1 as in tuning model above (in env.vars data set)
scale.env <- allVars.sc %>%
filter(SiteName %in% train.Names) %>% ## is the validation set (75% - select.percent)
dplyr::select(all_of(vars))
allVars.sc
scale.env <- allVars.sc %>%
filter(SiteName %in% train.Names, Period == "S1") %>% ## is the validation set (75% - select.percent)
dplyr::select(all_of(vars))
class(allVars.sc)
head(allVars.sc)
scale.env <- allVars.sc %>%
filter(SiteName %in% train.Names, period == "S1")
class(train.Names)
scale.env <- allVars.sc %>%
filter(period == "S1") %>% ## is the validation set (75% - select.percent)
dplyr::select(all_of(vars))
allVars.sc %>%
filter(period == "S1")
allVars.sc
tmp <- allVars.sc[allVars.sc$SiteName %in% train.Names,]
tmp <- allVars.sc[allVars.sc$SiteName %in% train.Names & period == "S1",]
tmp <- allVars.sc[allVars.sc$SiteName %in% train.Names & allVars.sc$period == "S1",]
tmp <- allVars.sc[allVars.sc$SiteName %in% train.Names & allVars.sc$period == "S1",vars]
scale.env <- allVars.sc[allVars.sc$SiteName %in% train.Names & allVars.sc$period == "S1",vars]
head(scale.env)
head(allVars.sc)
str(allVars.sc)
library(dplyr)
any(sapply(scale.env, function(x) any(is.na(x)))) # should be FALSE - no NAs
newData.sc <- newData.sc %>%
dplyr::select(all_of(vars))
# spatial data - just sites in model data, as above
scale.XY <- xy.sites.sc %>%
filter(SiteName %in% env.vars$SiteName, period == "S1") %>%
dplyr::select(c("UTM_E", "UTM_N"))%>%
as.matrix()
xy.sites.sc
scale.XY <- xy.sites.sc %>%
filter(SiteName %in% train.Names, period == "S1")
scale.xy <- as.matrix(xy.sites.sc[xy.sites.sc$SiteName %in% train.Names & period == "S1", c("UTM_E", "UTM_N")])
scale.xy <- as.matrix(xy.sites.sc[xy.sites.sc$SiteName %in% train.Names & xy.sites.sc$period == "S1", c("UTM_E", "UTM_N")])
View(scale.xy)
## Also filter S1 as in tuning model above (in env.vars data set)
scale.env <- allVars.sc %>%
filter(SiteName %in% !!train.Names, period == "S1") %>% ## is the validation set (75% - select.percent)
dplyr::select(all_of(vars))
load("D:/CD/UEA/Oregon/gis/processed_gis_data/sample_sites.rdata")
setwd("D:/CD/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
gis_out <- "D:/CD/UEA/Oregon/gis/processed_gis_data"
## load raster templates
load(file.path(gis_out, "templateRaster.rdata")) ## r, indNA aoi.pred.sf, r.aoi.pred - reduced area for plotting
setwd("D:/CD/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
baseFolder <- "code_sjSDM/r20210627a"
resFolder <- file.path(baseFolder, "results")
plotsFolder <- file.path(baseFolder, "plots")
abund <- "pa"
dir(resFolder)
## Load ordination results
load(file.path(resFolder, "ord_tsne_res.rdata")) #
plot(tsne$Y, pch = ".", asp = 1)
makeR <- function(r, siteScores, NAs) {
rSites <- raster(r)
rSites[] <- NA
rSites[NAs] <- siteScores
rSites
}
rtsne1 <- makeR(r, tsne$Y[,1], NAs)
rtsne2 <- makeR(r, tsne$Y[,2], NAs)
library(raster)
rtsne1 <- makeR(r, tsne$Y[,1], NAs)
rtsne2 <- makeR(r, tsne$Y[,2], NAs)
load(file.path(resFolder, "ord_tsne_res_30.rdata")) #
plot(tsne$Y, pch = ".", asp = 1)
rtsne1_5c <- makeR(r, tsne$Y[,1], NAs)
rtsne2_5c <- makeR(r, tsne$Y[,2], NAs)
stck <- raster::stack(rtsne1, rtsne2, rtsne1_5c, rtsne2_5c)
pdf(file.path(plotsFolder, "ordination_plots_tsne.pdf"), width = 8, height = 8)
plot(stck)
dev.off()
