alldata1 = cbind(dplyr::select(alldata1, -contains('__')), a)
dim(alldata1)
alldata1$lg_DistStream <- log(alldata1$distToStream_m + 0.001)
alldata1$lg_DistRoad <- log(alldata1$distToRoad_m + 0.001)
alldata1$lg_YrsDisturb <- log(alldata1$YrsSinceDist + 0.001)
non.sp.cols <- colnames(alldata1)[!grepl("__", colnames(alldata1))]
395-268
non.sp.cols
head(alldata1[,non.sp.cols])
summary((alldata1[,non.sp.cols]))
newvars <- c("be10", "tri", "slope", "Nss", "Ess", "ht", "ht.r250", "ht.r500", "ht.r1k", "cov2_4", "cov2_4.r250", "cov2_4.r500", "cov2_4.r1k", "cov4_16", "cov4_16.r250", "cov4_16.r500", "cov4_16.r1k", "be500", "mTopo", "cut.r1k.pt", "insideHJA", "minT_annual", "maxT_annual", "precipitation_mm", "lg_DistStream", "lg_DistRoad", "lg_YrsDisturb", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_p25", "l_rumple")
all.env.vars <- alldata1[,newVars]
all.env.vars <- alldata1[,newvars]
save(all.env.vars, file = "Hmsc_CD/working_cd/checkData.rdata")
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
library(dplyr)
# # model settings:
abund <- "pa"
device <- "gpu"
iter <- 150L
sampling <- 5000L
## Number of samples from tuning grid - random search
noSteps <- 1000
# no of CV folds
k <- 5
# timings...
# models take approx to 0.28 mins run (Run time 00:36:11 for 125 models)
noSteps * k * 0.28/60
# Storage
# About ~600k per model .rds -- in GB
noSteps * k * 600  / 1048576
# for testing on cpu
# device <- "cpu"
# iter <- 10L
# sampling <- 100L
# noSteps <- 5
### 1. Get data from github #####
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- paste0("outputs_minimap2_",
minimaprundate,"_",
samtoolsfilter,"_",
samtoolsqual,
"_kelpie",
kelpierundate,
"_",
primer,
"_vsearch97")
datFile <- paste0("sample_by_species_table_",
samtoolsfilter,
"_minimap2_",
minimaprundate,
"_kelpie",
kelpierundate,
"_FSL_qp.csv")
# file path:
fn <- file.path(gitHub, outputidxstatstabulatefolder, datFile)
# what file am i using?
basename(fn)
# when was it modified? - only if stored locally.
file.mtime(fn)
# read complete data set
otuenv <- read.csv(fn, stringsAsFactors = FALSE, na.strings = "NA")
# Filter M1S1
trap <- "M1"
period <- "S1"
otuenv <- otuenv %>%
dplyr::filter(trap == trap[[1]] & period == period[[1]])
# clean up
rm(datFile, gitHub, kelpierundate, minimaprundate, outputidxstatstabulatefolder, period, primer, samtoolsfilter, samtoolsqual, trap, fn)
# keep OTUs with >=5 incidences
minocc <- 5 # set to high number (e.g. 20) for testing
otu.qp.csv <- otuenv %>% dplyr::select(contains("__")) ## file above is already qp
otu.qp.csv <- otu.qp.csv[ , colSums(otu.qp.csv > 0) >= minocc]
# convert to presence/absence data
otu.pa.csv <- otu.qp.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv)) == minocc # should be TRUE
# rm(minocc)
# remove OTUs, XY, and normalised NDVI and EVI
# average, optionally log, select, and scale env covariates
env.vars <- otuenv %>%
dplyr::select(!contains("__"), UTM_E, UTM_N, -starts_with("nor")) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"),
elevation_m = elevation_f * 0.3048, ## convert to metres
canopyHeight_m = canopyHeight_f * 0.3048,
B1_median = apply(across(starts_with("B1_")), 1, median),
B2_median = apply(across(starts_with("B2_")), 1, median),
B3_median = apply(across(starts_with("B3_")), 1, median),
B4_median = apply(across(starts_with("B4_")), 1, median),
B5_median = apply(across(starts_with("B5_")), 1, median),
B6_median = apply(across(starts_with("B6_")), 1, median),
B7_median = apply(across(starts_with("B7_")), 1, median),
B10_median = apply(across(starts_with("B10_")), 1, median),
B11_median = apply(across(starts_with("B11_")), 1, median),
lg_DistStream = log(distToStream_m + 0.001),
lg_DistRoad = log(distToRoad_m + 0.001),
lg_YrsDisturb = log(YrsSinceDist + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001)) %>%
mutate(
#across(where(is.numeric), scale), # scale here # scale when defining models etc.
clearcut = factor(clearcut),
insideHJA = factor(insideHJA))
#dplyr::select(-uniqueID)
# str(env.vars)
# head(env.vars)
# old vars
oldVars <- c("insideHJA", "elevation_f", "canopyHeight_f", "minT_annual", "precipitation_mm", "distToRoad_m", "distToStream_m", "YrsSinceDist", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_Cover_2m_max", "l_Cover_2m_4m", "l_Cover_4m_16m", "l_p25", "l_p95", "l_rumple")
# new vars
newvars <- c("be10", "tri", "slope", "Nss", "Ess", "ht", "ht.r250", "ht.r500", "ht.r1k", "cov2_4", "cov2_4.r250", "cov2_4.r500", "cov2_4.r1k", "cov4_16", "cov4_16.r250", "cov4_16.r500", "cov4_16.r1k", "be500", "mTopo", "cut.r1k.pt", "insideHJA", "minT_annual", "maxT_annual", "precipitation_mm", "lg_DistStream", "lg_DistRoad", "lg_YrsDisturb", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_p25", "l_rumple")
all.env.vars2 <- env.vars[,newvars]
save(all.env.vars2, file= "Hmsc_CD/working_cd/compareData2")
save(all.env.vars2, file= "../working_cd/compareData2")
save(all.env.vars2, file= "../working_cd/compareData2.rdata")
save(all.env.vars2, file= "../working_cd/checkData2.rdata")
save(all.env.vars2, otu.pa.csv2, file= "../working_cd/checkData2.rdata")
otu.pa.csv2 <- otu.pa.csv
save(all.env.vars2, otu.pa.csv2, file= "../working_cd/checkData2.rdata")
wd <- here::here()
wd # "J:/UEA/gitHRepos/HJA_analyses_Kelpie"  local
setwd(wd)
samtoolsfilter = "F2308" # F2308 filter only
samtoolsqual = "q48"
minimaprundate = 20200929
kelpierundate = 20200927
primer = "BF3BR2"
minocc = 5
abund = 'pa' # pa , qp    # !!! change accordingly
trap <- "M1"; period = "S1"
date.model.run = 20210119   # !!! change accordingly
outputidxstatstabulatefolder = glue("outputs_minimap2_{minimaprundate}_{samtoolsfilter}_{samtoolsqual}_kelpie{kelpierundate}_{primer}_vsearch97")
outputpath = glue('Kelpie_maps/{outputidxstatstabulatefolder}')
sjsdmV = '0.1.3.9000' # package version
# names for graph
sjsdmVfolder = glue('sjsdm-{sjsdmV}')
# ..... load data ......
# what file am I loading?
basename(file.path(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')))
# when was it last modified?
file.mtime(file.path(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')))
# "2021-02-02 11:11:23 GMT"
alldata = read.csv(file.path(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')), header=T, sep=',', stringsAsFactors = F, na.strings='NA')
dim(alldata)
names(alldata)[1:10]
# ..... select trap, session .....
trap; period
alldata1 = subset(alldata, trap == 'M1' & period == 'S1')
dim(alldata1)
# select species with minocc
a = alldata1 %>% dplyr::select(contains('__'))
a = a[,which(vegan::specnumber(a, MARGIN=2)>=minocc)]
dim(a)
# join species back to data
alldata1 = cbind(dplyr::select(alldata1, -contains('__')), a)
dim(alldata1)
## Log the
# lg_DistStream = log(distToStream_m + 0.001),
# lg_DistRoad = log(distToRoad_m + 0.001),
# lg_YrsDisturb = log(YrsSinceDist + 0.001),
alldata1$lg_DistStream <- log(alldata1$distToStream_m + 0.001)
alldata1$lg_DistRoad <- log(alldata1$distToRoad_m + 0.001)
alldata1$lg_YrsDisturb <- log(alldata1$YrsSinceDist + 0.001)
# check data
# non species names
non.sp.cols <- colnames(alldata1)[!grepl("__", colnames(alldata1))]
395-268
non.sp.cols
head(alldata1[,non.sp.cols])
summary((alldata1[,non.sp.cols]))
# data is not scaled here..
# ... 80% of data as training ...
# random selection
num.sample = dim(a)[1]
select.percent = .8
ssdd = 100 		# please keep this value to make results comparable!
set.seed(ssdd)
a = base::sample(1:num.sample, round(num.sample*select.percent))
a
# [1] 74 78 23 70  4 55 85  7 81 83 43 61 12 51 72 18 25  2 75 68 69 52 48 32 21
# [26] 27 39 57 16 11 67 71  6 29 45 30 53 79 86 31 33 49 82 28 47 41 87 42 24 80
# [51]  1  9 20 14 35 40  3 34 84 19 46 63 44 36 26  5 15 22 58 76
otu = dplyr::select(alldata1, contains('__'))
newvars <- c("be10", "tri", "slope", "Nss", "Ess", "ht", "ht.r250", "ht.r500", "ht.r1k", "cov2_4", "cov2_4.r250", "cov2_4.r500", "cov2_4.r1k", "cov4_16", "cov4_16.r250", "cov4_16.r500", "cov4_16.r1k", "be500", "mTopo", "cut.r1k.pt", "insideHJA", "minT_annual", "maxT_annual", "precipitation_mm", "lg_DistStream", "lg_DistRoad", "lg_YrsDisturb", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_p25", "l_rumple")
all.env.vars <- alldata1[,newvars]
save(all.env.vars, otu, file = "Hmsc_CD/working_cd/checkData.rdata")
load("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/working_cd/checkData.rdata")
load("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/working_cd/checkData2.rdata")
identical(otu, otu.pa.csv2)
identical(all.env.vars, all.env.vars2)
head(all.env.vars)
all.env.vars[1:10, 1:10]
all.env.vars2[1:10, 1:10]
colMeans(all.env.vars)
colSums(otu) == colSums(otu.pa.csv2)
colSums(otu)
colSums(otu.pa.csv2)
otu = as.data.frame((otu>0)*1)
colSums(otu) == colSums(otu.pa.csv2)
all(colSums(otu) == colSums(otu.pa.csv2))
all.env.vars[1:10, 1:10]
all.env.vars2[1:10, 1:10]
apply(all.env.vars, 2, mean)
apply(all.env.vars2, 2, mean)
warnings()
sapply(all.env.vars2, 2, mean)
sapply(all.env.vars2, mean)
sapply(all.env.vars, mean) == sapply(all.env.vars2, mean)
all(sapply(all.env.vars, mean) == sapply(all.env.vars2, mean))
sapply(all.env.vars, mean) == sapply(all.env.vars2, mean)
all.env.vars[1:10, 1:5]
all.env.vars2[1:10, 1:5]
all.env.vars[1:10, 10:15]
all.env.vars2[1:10, 10:15]
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
pa <- read.csv("results_sjSDM/tuning_YL_pa_newVars/manual_tuning_sjsdm_newVars_S1_M1_pa_min5_envDNN_20210125.csv")
pa.best <- pa[which.max(pa$AUC.test),,drop = T]
pa.best
pa.best$hidden
pa.best
lambda.env = pa.best$lambda.env
alpha.env = pa.best$alpha.env
lambda.sp = pa.best$lambda.sp
alpha.sp =  pa.best$alpha.sp
hidden <- list(c(50L,50L,10L), c(25L,25L,10L))
hidden.ind = pa.best$hidden
acti.sp = 'relu'
drop = pa.best$drop
tune.grid <- expand.grid(lambda.env = lambda.env, alpha.env= alpha.env, lambda.sp = lambda.sp,
alpha.sp = alpha.sp, hidden.ind = hidden.ind,
drop= drop, acti.sp = acti.sp, stringsAsFactors = FALSE)
head(tune.grid)
resFolder <-"results_sjSDM/test"
if(!dir.exists(resFolder)) dir.create(resFolder)
resFolder <-"results_sjSDM/test5"
if(!dir.exists(resFolder)) dir.create(resFolder)
abund <- "pa"
device <- "gpu"
iter <- 150L
sampling <- 5000L
## Number of samples from tuning grid - random search
noSteps <- 1
# no of CV folds
k <- 5
# timings...
# models take approx to 0.28 mins run (Run time 00:36:11 for 125 models)
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- paste0("outputs_minimap2_",
minimaprundate,"_",
samtoolsfilter,"_",
samtoolsqual,
"_kelpie",
kelpierundate,
"_",
primer,
"_vsearch97")
datFile <- paste0("sample_by_species_table_",
samtoolsfilter,
"_minimap2_",
minimaprundate,
"_kelpie",
kelpierundate,
"_FSL_qp.csv")
# file path:
fn <- file.path(gitHub, outputidxstatstabulatefolder, datFile)
# what file am i using?
basename(fn)
# when was it modified? - only if stored locally.
file.mtime(fn)
# read complete data set
otuenv <- read.csv(fn, stringsAsFactors = FALSE, na.strings = "NA")
# Filter M1S1
trap <- "M1"
period <- "S1"
otuenv <- otuenv %>%
dplyr::filter(trap == trap[[1]] & period == period[[1]])
# clean up
rm(datFile, gitHub, kelpierundate, minimaprundate, outputidxstatstabulatefolder, period, primer, samtoolsfilter, samtoolsqual, trap, fn)
# keep OTUs with >=5 incidences
minocc <- 5 # set to high number (e.g. 20) for testing
otu.qp.csv <- otuenv %>% dplyr::select(contains("__")) ## file above is already qp
otu.qp.csv <- otu.qp.csv[ , colSums(otu.qp.csv > 0) >= minocc]
# convert to presence/absence data
otu.pa.csv <- otu.qp.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv)) == minocc # should be TRUE
env.vars <- otuenv %>%
dplyr::select(!contains("__"), UTM_E, UTM_N, -starts_with("nor")) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"),
elevation_m = elevation_f * 0.3048, ## convert to metres
canopyHeight_m = canopyHeight_f * 0.3048,
B1_median = apply(across(starts_with("B1_")), 1, median),
B2_median = apply(across(starts_with("B2_")), 1, median),
B3_median = apply(across(starts_with("B3_")), 1, median),
B4_median = apply(across(starts_with("B4_")), 1, median),
B5_median = apply(across(starts_with("B5_")), 1, median),
B6_median = apply(across(starts_with("B6_")), 1, median),
B7_median = apply(across(starts_with("B7_")), 1, median),
B10_median = apply(across(starts_with("B10_")), 1, median),
B11_median = apply(across(starts_with("B11_")), 1, median),
lg_DistStream = log(distToStream_m + 0.001),
lg_DistRoad = log(distToRoad_m + 0.001),
lg_YrsDisturb = log(YrsSinceDist + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001)) %>%
# dplyr::select(uniqueID, clearcut,insideHJA,oldGrowthIndex, elevation_m, canopyHeight_m, precipitation_mm, minT_annual,
#               maxT_annual, mean.NDVI, mean.EVI, mean.green, mean.wet, mean.bright, l_p25, l_p95, l_rumple, B1_median,
#               B2_median,B3_median,B4_median,B5_median,B6_median,B7_median,B10_median,B11_median,lg_DistStream,
#               lg_DistRoad, lg_YrsDisturb, lg_cover2m_max, lg_cover2m_4m, lg_cover4m_16m, l_Cover_2m_4m,l_Cover_4m_16m,
#               be10, tri, slope, twi, Nss, Ess, ht, ht.r250, ht.r500, ht.r1k, cov2_4, cov2_4.r250, cov2_4.r500, cov2_4.r1k,
#               cov4_16, cov4_16.r250, cov4_16.r500, cov4_16.r1k, be500, mTopo, cut.r1k.pt,B1_20180717, B2_20180717,
#               B3_20180717, B4_20180717, B5_20180717, B6_20180717, B7_20180717, B10_20180717, B11_20180717, NDVI_20180717,
#               EVI_20180717, B_20180717, G_20180717, W_20180717) %>%
mutate(
#across(where(is.numeric), scale), # scale here # scale when defining models etc.
clearcut = factor(clearcut),
insideHJA = factor(insideHJA))
#dplyr::select(-uniqueID)
# str(env.vars)
newvars <- c("be10", "tri", "slope", "Nss", "Ess", "ht", "ht.r250", "ht.r500", "ht.r1k", "cov2_4", "cov2_4.r250", "cov2_4.r500", "cov2_4.r1k", "cov4_16", "cov4_16.r250", "cov4_16.r500", "cov4_16.r1k", "be500", "mTopo", "cut.r1k.pt", "insideHJA", "minT_annual", "maxT_annual", "precipitation_mm", "lg_DistStream", "lg_DistRoad", "lg_YrsDisturb", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_p25", "l_rumple")
set.seed(100)
fold.id <- sample(rep_len(1:k, length.out = nrow(otu.pa.csv)))
table(fold.id)
tune.rows <- sample(1:nrow(tune.grid), size = noSteps, replace = FALSE)
tune.rows
tune.results <- data.frame(tr = 1:noSteps,
tune.grid[tune.rows,],
lambda.bio = pa.best$lambda.bio,
alpha.bio = pa.best$alpha.bio,
loglike = NA,
loss= NA,
AUC.explain = NA,
AUC.test = NA)
str(tune.results)
tune.results <- cbind(tune.results[rep(seq(noSteps), k),], k = rep(1:k, each = noSteps))
head(tune.results)
rm(lambda.env, alpha.env, lambda.sp, alpha.sp, hidden.ind, drop, sample.bio, acti.sp)
if(abund == "pa") {
Y <- otu.pa.csv
family <- stats::binomial('probit') } else {
if(abund ==  "qp") {
Y <- otu.qp.csv
# family <- stats::binomial()
} else stop("check abund")
}
i= 1
env.train <- env.vars[fold.id != i, newvars]
env.test <- env.vars[fold.id == i, newvars]
XY.train <- env.vars[fold.id != i, c("UTM_E", "UTM_N")]
XY.test <- env.vars[fold.id == i, c("UTM_E", "UTM_N")]
s.otu.train <- as.matrix(Y[fold.id != i,])
s.otu.test <- as.matrix(Y[fold.id == i,])
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
resFolder <-"code_sjSDM/r20210217/results"
if(!dir.exists(resFolder)) dir.create(resFolder)
abund <- "pa"
device <- "gpu"
iter <- 150L
sampling <- 5000L
## Number of samples from tuning grid - random search
noSteps <- 500
# no of CV folds
k <- 5
# timings...
# models take approx to 0.28 mins run (Run time 00:36:11 for 125 models)
noSteps * k * 0.28/60
# Storage
# About ~600k per model .rds -- in GB
noSteps * k * 600  / 1048576
# for testing on cpu
# device <- "cpu"
# iter <- 10L
# sampling <- 100L
# noSteps <- 5
### 1. Get data from github #####
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- paste0("outputs_minimap2_",
minimaprundate,"_",
samtoolsfilter,"_",
samtoolsqual,
"_kelpie",
kelpierundate,
"_",
primer,
"_vsearch97")
datFile <- paste0("sample_by_species_table_",
samtoolsfilter,
"_minimap2_",
minimaprundate,
"_kelpie",
kelpierundate,
"_FSL_qp.csv")
# file path:
fn <- file.path(gitHub, outputidxstatstabulatefolder, datFile)
# what file am i using?
basename(fn)
# when was it modified? - only if stored locally.
file.mtime(fn)
# read complete data set
otuenv <- read.csv(fn, stringsAsFactors = FALSE, na.strings = "NA")
# Filter M1S1
trap <- "M1"
period <- "S1"
otuenv <- otuenv %>%
dplyr::filter(trap == trap[[1]] & period == period[[1]])
# clean up
rm(datFile, gitHub, kelpierundate, minimaprundate, outputidxstatstabulatefolder, period, primer, samtoolsfilter, samtoolsqual, trap, fn)
# keep OTUs with >=5 incidences
minocc <- 5 # set to high number (e.g. 20) for testing
otu.qp.csv <- otuenv %>% dplyr::select(contains("__")) ## file above is already qp
otu.qp.csv <- otu.qp.csv[ , colSums(otu.qp.csv > 0) >= minocc]
# convert to presence/absence data
otu.pa.csv <- otu.qp.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv)) == minocc # should be TRUE
# rm(minocc)
# remove OTUs, XY, and normalised NDVI and EVI
# average, optionally log, select, and scale env covariates
env.vars <- otuenv %>%
dplyr::select(!contains("__"), UTM_E, UTM_N, -starts_with("nor")) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"),
elevation_m = elevation_f * 0.3048, ## convert to metres
canopyHeight_m = canopyHeight_f * 0.3048,
B1_median = apply(across(starts_with("B1_")), 1, median),
B2_median = apply(across(starts_with("B2_")), 1, median),
B3_median = apply(across(starts_with("B3_")), 1, median),
B4_median = apply(across(starts_with("B4_")), 1, median),
B5_median = apply(across(starts_with("B5_")), 1, median),
B6_median = apply(across(starts_with("B6_")), 1, median),
B7_median = apply(across(starts_with("B7_")), 1, median),
B10_median = apply(across(starts_with("B10_")), 1, median),
B11_median = apply(across(starts_with("B11_")), 1, median),
lg_DistStream = log(distToStream_m + 0.001),
lg_DistRoad = log(distToRoad_m + 0.001),
lg_YrsDisturb = log(YrsSinceDist + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001)) %>%
mutate(
#across(where(is.numeric), scale), # scale here # scale when defining models etc.
clearcut = factor(clearcut),
insideHJA = factor(insideHJA))
#dplyr::select(-uniqueID)
# str(env.vars)
# head(env.vars)
# old vars
oldVars <- c("insideHJA", "elevation_f", "canopyHeight_f", "minT_annual", "precipitation_mm", "distToRoad_m", "distToStream_m", "YrsSinceDist", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_Cover_2m_max", "l_Cover_2m_4m", "l_Cover_4m_16m", "l_p25", "l_p95", "l_rumple")
# new vars
newvars <- c("be10", "tri", "slope", "Nss", "Ess", "ht", "ht.r250", "ht.r500", "ht.r1k", "cov2_4", "cov2_4.r250", "cov2_4.r500", "cov2_4.r1k", "cov4_16", "cov4_16.r250", "cov4_16.r500", "cov4_16.r1k", "be500", "mTopo", "cut.r1k.pt", "insideHJA", "minT_annual", "maxT_annual", "precipitation_mm", "lg_DistStream", "lg_DistRoad", "lg_YrsDisturb", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_p25", "l_rumple")
# all.env.vars2 <- env.vars[,newvars]
# otu.pa.csv2 <- otu.pa.csv
# save(all.env.vars2, otu.pa.csv2, file= "../working_cd/checkData2.rdata")
set.seed(100)
fold.id <- sample(rep_len(1:k, length.out = nrow(otu.pa.csv)))
table(fold.id)
lambda.env = seq(0,.3, length.out=4)	# .1
alpha.env = seq(.7,1, length.out=4)		# .9
lambda.sp = seq(0,1, length.out=5)	# .1 ## Changed to 5
alpha.sp =  seq(0,1, length.out=5)	# .5 ## Changed to 5
hidden <- list(c(50L,50L,10L), c(25L,25L,10L))
hidden.ind = seq_along(hidden)
acti.sp = 'relu'
drop = seq(.1,.5, length.out=3) # .3
sample.bio = seq(0,1,length.out=11)
## Make grid of priority tune parameters, choose, from these in sampling, then add lower priority parameters
tune.grid <- expand.grid(lambda.env = lambda.env, alpha.env= alpha.env, lambda.sp = lambda.sp,
alpha.sp = alpha.sp, hidden.ind = hidden.ind,
drop= drop, acti.sp = acti.sp, stringsAsFactors = FALSE)
head(tune.grid)
#  no models
# 4*4*7*7*2*3 *11
# data storage
tune.rows <- sample(1:nrow(tune.grid), size = noSteps, replace = FALSE)
# add in lambda.bio and alpha.bio from random samples of sample.bio, and add results columns
tune.results <- data.frame(tr = 1:noSteps,
tune.grid[tune.rows,],
lambda.bio = sample(sample.bio, size = noSteps, replace = TRUE),
alpha.bio = sample(sample.bio, size = noSteps, replace = TRUE),
loglike = NA,
loss= NA,
AUC.explain = NA,
AUC.test = NA)
str(tune.results)
# Add in k, to keep all cross validation runs. Average later.
tune.results <- cbind(tune.results[rep(seq(noSteps), k),], k = rep(1:k, each = noSteps))
head(tune.results)
# clean up
rm(lambda.env, alpha.env, lambda.sp, alpha.sp, hidden.ind, drop, sample.bio, acti.sp)
if(abund == "pa") {
Y <- otu.pa.csv
family <- stats::binomial('probit') } else {
if(abund ==  "qp") {
Y <- otu.qp.csv
# family <- stats::binomial()
} else stop("check abund")
}
