site.chk <- otuenv %>%
filter(period == "S1") %>%
select(c(UTM_N, UTM_E, SiteName, trap)) %>%
tidyr::pivot_wider(names_from = trap, values_from = trap, values_fn = length)%>%
mutate(numTrap = rowSums(select(., M1, M2), na.rm = TRUE))
site.chk
## shuffle order of sites (seed is set globally at start - so can be changed for different runs)
site.chk <- site.chk[sample(1:nrow(site.chk)),]
# sum numTraps until percent training is reached
## (group numbers can be out by 1 due to summing 1s or 2s - could use the while loop to correct this)
select.percent <- 0.75
num.train <- round(sum(site.chk$numTrap)*select.percent) # training split is total no of samples * select %
sum(site.chk$numTrap) - num.train
# num.test = num.point - num.train
site.chk$cumsum <- cumsum(site.chk$numTrap)
train.Names <- site.chk$SiteName[which(site.chk$cumsum <= num.train)]
test.Names <- site.chk$SiteName[which(site.chk$cumsum > num.train)]
# check
length(train.Names); length(test.Names)
sum(site.chk$numTrap[site.chk$SiteName %in% train.Names]); sum(site.chk$numTrap[site.chk$SiteName %in% test.Names])
rm(num.train)
## get Species columns by M1 and M2, with minocc calculated per trap
## can choose below whether to include just species shared between M1 and M2
spM <- otuenv %>%
dplyr::filter(period == "S1" & SiteName %in% train.Names) %>%
dplyr::select(SiteName, trap, contains("__")) %>%
tidyr::pivot_longer(cols = contains("__"), names_to = "OTU", values_drop_na = FALSE) %>%
mutate(value = value>0) %>% # change to PA
group_by(OTU, trap) %>%
summarise(nSites = sum(value, na.rm = T)) %>% # Number of sites at which present
filter(nSites >= minocc) %>% # filter by minocc
ungroup() %>%
tidyr::pivot_wider(names_from = trap, values_from = nSites, values_fn = function(x) sum(x)>0) %>%
#filter(M1) %>% # CHOOOSE HERE FOR SINGLE. OR SHARED TRAP SPECIES GFROUP: filter(M1 & M2)
# tidyr::separate(col = OTU, into = c("ID", "empty", "class", "order", "family",
#                                         "genus", "epithet", "BOLD", "BOLDID", "size"),
#                 remove = FALSE, sep = "_") %>%
# dplyr::filter(order == "Diptera")%>%
dplyr::select(OTU)
spM
nrow(spM)
### Create splits for training data into k folds for tuning
# make fold ids - same as above to split training/test
cv.chk <- site.chk %>%
filter(SiteName %in% train.Names)
## shuffle order of sites (seed is set globally)
cv.chk <- cv.chk[sample(1:nrow(cv.chk)),]
# sum numTraps until percent training is reached
out <- sum(cv.chk$numTrap)
by <- out%/%k
splits <- seq(1, (out+by - out%%by), by = by)
# num.test = num.point - num.train
cv.chk$cumsum <- cumsum(cv.chk$numTrap)
cv.chk$fold.id <- as.numeric(cut(cv.chk$cumsum, breaks = splits, include.lowest = T, labels = 1:k, right = F))
head(cv.chk)
otu.folds <- otuenv %>%
dplyr::filter(period == "S1" & SiteName %in% train.Names) %>% ## filter for sites in trainig/validation data
left_join(cv.chk)
## Check
table(otu.folds$fold.id)
table(otu.folds[,c("fold.id", "SiteName")])
fold.id <- otu.folds$fold.id
rm(out, by, cv.chk, splits, site.chk)
###  filter species here to those in sp.M1m2$OTU - already filtered for minocc
## training / validation data set
otu.qp.csv <- select(otu.folds, spM$OTU)## species filter for minocc or taxon
# convert to presence/absence data
otu.pa.csv <- otu.qp.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv))  # should be minocc - (only if we filter on minocc for training set separately)
otu.qp.csv.test <- otuenv %>%
dplyr::filter(period == "S1" & SiteName %in% test.Names) %>% ## filter for sites in trainig/validation data
dplyr::select(spM$OTU) ## Only use species selected for minocc or taxon on training
## some species with no presences....
sum(apply(otu.qp.csv.test, 2, function(x) sum(x > 0))==0)
# convert to presence/absence data
otu.pa.csv.test <- otu.qp.csv.test
otu.pa.csv.test[otu.pa.csv.test > 0] <- 1
min(colSums(otu.pa.csv.test)) # - some species with no presences....
# clean up
rm(datFile, gitHub, kelpierundate, minimaprundate,
outputidxstatstabulatefolder, primer, samtoolsfilter, samtoolsqual, fn, spM)
## Load new version of vars
load("data/envVars.rdata")
env.vars <- otuenv %>%
dplyr::filter(period == "S1") %>%
dplyr::select(trap, period, UTM_E, UTM_N, SiteName) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_")) %>%
left_join(y = allVars, by = "SiteName") %>%
mutate(lg_DistStream = log(DistStream + 0.001),
lg_DistRoad = log(DistRoad + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001))
# "insideHJA","cut_msk", "cut_40msk",
all.vars <- c("ht30", "gt4_r30", "gt4_250", "gt4_500", "cut_r1k", "cut_r500", "cut_r250", "cut40_r1k", "cut40_r500",
"cut40_r250", "be30", "tri30","slope30", "Nss30", "Ess30", "twi30", "tpi250", "tpi500", "tpi1k", "l_p25",
"l_p95", "l_rumple", "ndmi_stdDev_r100","ndmi_stdDev_r250", "ndmi_stdDev_r500", "nbr_stdDev_r100",
"nbr_stdDev_r250", "nbr_stdDev_r500", "ndvi_p5_r100", "ndvi_p5_r250","ndvi_p5_r500", "ndvi_p50_r100",
"ndvi_p50_r250", "ndvi_p50_r500", "ndvi_p95_r100", "ndvi_p95_r250", "ndvi_p95_r500", "ndmi_p5_r100",
"ndmi_p5_r250", "ndmi_p5_r500", "ndmi_p50_r100", "ndmi_p50_r250", "ndmi_p50_r500", "ndmi_p95_r100",
"ndmi_p95_r250", "ndmi_p95_r500","LC08_045029_20180726_B1", "LC08_045029_20180726_B3",
"LC08_045029_20180726_B4", "LC08_045029_20180726_B5", "LC08_045029_20180726_B7",
"LC08_045029_20180726_B10", "lg_DistStream", "lg_DistRoad", "lg_cover2m_max",
"lg_cover2m_4m", "lg_cover4m_16m")
sum(!complete.cases(env.vars[,all.vars]))
head(env.vars[,all.vars])
source("https://raw.githubusercontent.com/Cdevenish/R-Material/master/Functions/Eco/viffer.r")
vif <- viffer(env.vars[,all.vars], keep = c("be30"))
vif <- viffer(env.vars[,all.vars], keep = c("be30"), v = 8)
vif <- viffer(env.vars[,all.vars], keep = c("be30"), z = 8)
vif.vars <- rownames(vif)
vars <- c(vif.vars, "insideHJA")
varsName <- "vars11"
#
# rm(dd, varrem, vif, order)
rm(vif.vars, vif)
## separate env.vars into evaluation and non evaluation data sets
env.vars.test <- env.vars[env.vars$SiteName %in% test.Names, ]
env.vars <- env.vars[env.vars$SiteName %in% train.Names, ]
# check names
all(vars %in% colnames(env.vars))
## Save model data
save(otu.pa.csv, otu.qp.csv, otu.pa.csv.test, otu.qp.csv.test, otuenv, env.vars, env.vars.test,
spChoose, test.Names, train.Names, select.percent,
k, minocc, noSteps, vars, varsName, abund, device, iter, sampling,
file = file.path(resFolder, paste0("modelData_",abund,".rdata")))
# viffer function on github
dd = env.vars[,all.vars]
varrem = 'a'; maxvif = 100
names(dd)
while  (maxvif>=8 ) {
if (varrem!='a') { dd = dplyr::select(dd, -all_of(varrem)) }
vif = corvif(dd)
order = order(vif$GVIF, decreasing=F)
vif.count = data.frame(var=names(dd)[order], vif=vif[order,])
maxvif = max(vif.count$vif)
varrem = as.character(vif.count$var[ncol(dd)])
print(c(ncol(dd), varrem))
}
vif.vars <- vif.count$var
dd = env.vars[,all.vars[!all.vars %in% "be30"]]
# viffer function on github
dd = env.vars[,all.vars]
dd = env.vars[,all.vars[!all.vars %in% "be30"]]
varrem = 'a'; maxvif = 100
names(dd)
while  (maxvif>=8 ) {
if (varrem!='a') { dd = dplyr::select(dd, -all_of(varrem)) }
vif = corvif(dd)
order = order(vif$GVIF, decreasing=F)
vif.count = data.frame(var=names(dd)[order], vif=vif[order,])
maxvif = max(vif.count$vif)
varrem = as.character(vif.count$var[ncol(dd)])
print(c(ncol(dd), varrem))
}
vif.vars <- vif.count$var
vif <- viffer(env.vars[,all.vars], keep = c("be30"), z = 8)
vif.vars2 <- rownames(vif)
warnings()
vif.vars2
vif.vars
setdiff(vif.vars, vif.vars2)
# viffer function on github
dd = env.vars[,all.vars]
varrem = 'a'; maxvif = 100
names(dd)
while  (maxvif>=8 ) {
if (varrem!='a') { dd = dplyr::select(dd, -all_of(varrem)) }
vif = corvif(dd)
order = order(vif$GVIF, decreasing=F)
vif.count = data.frame(var=names(dd)[order], vif=vif[order,])
maxvif = max(vif.count$vif)
varrem = as.character(vif.count$var[ncol(dd)])
print(c(ncol(dd), varrem))
}
vif.vars <- vif.count$var
View(corvif)
View(myvif)
View(viffer)
dd = env.vars[,all.vars]
#dd = env.vars[,all.vars[!all.vars %in% "be30"]]
varrem = 'a'; maxvif = 100
names(dd)
while  (maxvif>=8 ) {
if (varrem!='a') { dd = dplyr::select(dd, -all_of(varrem)) }
vif = corvif(dd)
order = order(vif$GVIF, decreasing=F)
vif.count = data.frame(var=names(dd)[order], vif=vif[order,])
maxvif = max(vif.count$vif)
varrem = as.character(vif.count$var[ncol(dd)])
print(c(ncol(dd), varrem))
}
vif.vars <- vif.count$var
vif <- viffer(env.vars[,all.vars], z = 8)
vif.vars2 <- rownames(vif)
setdiff(vif.vars, vif.vars2)
sort(vif.vars2)
sort(vif.vars)
library(dplyr)
set.seed(501)
resFolder <-"code_sjSDM/r20210716b/results"
if(!dir.exists(resFolder)) dir.create(resFolder, recursive = TRUE)
# source("data/vif_zuur.r")
## Updated to new vars, also changes to elevation_m, canopy_height_m  to _f.
# # model settings:
abund <- "pa"
spChoose <- "M1S1_v16"
device <- "gpu"
iter <- 170L
sampling <- 5000L
## Number of samples from tuning grid - random search
noSteps <- 1000
# no of CV folds
k <- 5
# timings...
# models take approx to 0.28 mins run (Run time 00:36:11 for 125 models)
# noSteps * k * 0.28/60
noSteps * k
# Storage
# About ~600k per model .rds -- in GB
# noSteps * k * 600  / 1048576
# for testing on cpu
# device <- "cpu"
# iter <- 10L
# sampling <- 100L
# noSteps <- 2
# k <- 2
### 1. Get data from github #####
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- paste0("outputs_minimap2_",
minimaprundate,"_",
samtoolsfilter,"_",
samtoolsqual,
"_kelpie",
kelpierundate,
"_",
primer,
"_vsearch97")
datFile <- paste0("sample_by_species_table_",
samtoolsfilter,
"_minimap2_",
minimaprundate,
"_kelpie",
kelpierundate,
"_FSL_qp.csv")
# file path:
fn <- file.path(gitHub, outputidxstatstabulatefolder, datFile)
# what file am i using?
basename(fn)
# when was it modified? - only if stored locally.
file.mtime(fn)
# read complete data set
otuenv <- read.csv(fn, stringsAsFactors = FALSE, na.strings = "NA")
# keep OTUs with >= minocc incidences AND with presnece at both M1 or M2
minocc <- 6 # set to high number (e.g. 20) for testing
### 2. Split data #####
## Split whole data set into Training/Validation  and hold out testing set
## Combine M1 and M2 data and split so that sites with both M1 and M2 samples are always in same split
# get list of sites with presence of M1 and M2 samples as columns
site.chk <- otuenv %>%
filter(period == "S1") %>%
select(c(UTM_N, UTM_E, SiteName, trap)) %>%
tidyr::pivot_wider(names_from = trap, values_from = trap, values_fn = length)%>%
mutate(numTrap = rowSums(select(., M1, M2), na.rm = TRUE))
site.chk
## shuffle order of sites (seed is set globally at start - so can be changed for different runs)
site.chk <- site.chk[sample(1:nrow(site.chk)),]
# sum numTraps until percent training is reached
## (group numbers can be out by 1 due to summing 1s or 2s - could use the while loop to correct this)
select.percent <- 0.75
num.train <- round(sum(site.chk$numTrap)*select.percent) # training split is total no of samples * select %
sum(site.chk$numTrap) - num.train
# num.test = num.point - num.train
site.chk$cumsum <- cumsum(site.chk$numTrap)
train.Names <- site.chk$SiteName[which(site.chk$cumsum <= num.train)]
test.Names <- site.chk$SiteName[which(site.chk$cumsum > num.train)]
# check
length(train.Names); length(test.Names)
sum(site.chk$numTrap[site.chk$SiteName %in% train.Names]); sum(site.chk$numTrap[site.chk$SiteName %in% test.Names])
rm(num.train)
### 3. Choose species by minocc, taxon, etc #####
## get Species columns by M1 and M2, with minocc calculated per trap
## can choose below whether to include just species shared between M1 and M2
spM <- otuenv %>%
dplyr::filter(period == "S1" & SiteName %in% train.Names) %>%
dplyr::select(SiteName, trap, contains("__")) %>%
tidyr::pivot_longer(cols = contains("__"), names_to = "OTU", values_drop_na = FALSE) %>%
mutate(value = value>0) %>% # change to PA
group_by(OTU, trap) %>%
summarise(nSites = sum(value, na.rm = T)) %>% # Number of sites at which present
filter(nSites >= minocc) %>% # filter by minocc
ungroup() %>%
tidyr::pivot_wider(names_from = trap, values_from = nSites, values_fn = function(x) sum(x)>0) %>%
#filter(M1) %>% # CHOOOSE HERE FOR SINGLE. OR SHARED TRAP SPECIES GFROUP: filter(M1 & M2)
# tidyr::separate(col = OTU, into = c("ID", "empty", "class", "order", "family",
#                                         "genus", "epithet", "BOLD", "BOLDID", "size"),
#                 remove = FALSE, sep = "_") %>%
# dplyr::filter(order == "Diptera")%>%
dplyr::select(OTU)
spM
nrow(spM)
### 4. Make cross validation folds #####
### Create splits for training data into k folds for tuning
# make fold ids - same as above to split training/test
cv.chk <- site.chk %>%
filter(SiteName %in% train.Names)
## shuffle order of sites (seed is set globally)
cv.chk <- cv.chk[sample(1:nrow(cv.chk)),]
# sum numTraps until percent training is reached
out <- sum(cv.chk$numTrap)
by <- out%/%k
splits <- seq(1, (out+by - out%%by), by = by)
# num.test = num.point - num.train
cv.chk$cumsum <- cumsum(cv.chk$numTrap)
cv.chk$fold.id <- as.numeric(cut(cv.chk$cumsum, breaks = splits, include.lowest = T, labels = 1:k, right = F))
head(cv.chk)
otu.folds <- otuenv %>%
dplyr::filter(period == "S1" & SiteName %in% train.Names) %>% ## filter for sites in trainig/validation data
left_join(cv.chk)
## Check
table(otu.folds$fold.id)
table(otu.folds[,c("fold.id", "SiteName")])
fold.id <- otu.folds$fold.id
rm(out, by, cv.chk, splits, site.chk)
###  filter species here to those in sp.M1m2$OTU - already filtered for minocc
## training / validation data set
otu.qp.csv <- select(otu.folds, spM$OTU)## species filter for minocc or taxon
# convert to presence/absence data
otu.pa.csv <- otu.qp.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv))  # should be minocc - (only if we filter on minocc for training set separately)
### 5. Make Test data set ######
otu.qp.csv.test <- otuenv %>%
dplyr::filter(period == "S1" & SiteName %in% test.Names) %>% ## filter for sites in trainig/validation data
dplyr::select(spM$OTU) ## Only use species selected for minocc or taxon on training
## some species with no presences....
sum(apply(otu.qp.csv.test, 2, function(x) sum(x > 0))==0)
# convert to presence/absence data
otu.pa.csv.test <- otu.qp.csv.test
otu.pa.csv.test[otu.pa.csv.test > 0] <- 1
min(colSums(otu.pa.csv.test)) # - some species with no presences....
# clean up
rm(datFile, gitHub, kelpierundate, minimaprundate,
outputidxstatstabulatefolder, primer, samtoolsfilter, samtoolsqual, fn, spM)
# remove OTUs, XY, and normalised NDVI and EVI
# average, optionally log, select, and scale env covariates
### 6. Load and filter predictors to data training/test sets ######
## Load new version of vars
load("data/envVars.rdata")
# head(allVars)
env.vars <- otuenv %>%
dplyr::filter(period == "S1") %>%
dplyr::select(trap, period, UTM_E, UTM_N, SiteName) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_")) %>%
left_join(y = allVars, by = "SiteName") %>%
mutate(lg_DistStream = log(DistStream + 0.001),
lg_DistRoad = log(DistRoad + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001))
# str(env.vars)
# head(env.vars)
# cat(paste(colnames(env.vars), collapse = '", "'))
# "insideHJA","cut_msk", "cut_40msk",
all.vars <- c("ht30", "gt4_r30", "gt4_250", "gt4_500", "cut_r1k", "cut_r500", "cut_r250", "cut40_r1k", "cut40_r500",
"cut40_r250", "be30", "tri30","slope30", "Nss30", "Ess30", "twi30", "tpi250", "tpi500", "tpi1k", "l_p25",
"l_p95", "l_rumple", "ndmi_stdDev_r100","ndmi_stdDev_r250", "ndmi_stdDev_r500", "nbr_stdDev_r100",
"nbr_stdDev_r250", "nbr_stdDev_r500", "ndvi_p5_r100", "ndvi_p5_r250","ndvi_p5_r500", "ndvi_p50_r100",
"ndvi_p50_r250", "ndvi_p50_r500", "ndvi_p95_r100", "ndvi_p95_r250", "ndvi_p95_r500", "ndmi_p5_r100",
"ndmi_p5_r250", "ndmi_p5_r500", "ndmi_p50_r100", "ndmi_p50_r250", "ndmi_p50_r500", "ndmi_p95_r100",
"ndmi_p95_r250", "ndmi_p95_r500","LC08_045029_20180726_B1", "LC08_045029_20180726_B3",
"LC08_045029_20180726_B4", "LC08_045029_20180726_B5", "LC08_045029_20180726_B7",
"LC08_045029_20180726_B10", "lg_DistStream", "lg_DistRoad", "lg_cover2m_max",
"lg_cover2m_4m", "lg_cover4m_16m")
sum(!complete.cases(env.vars[,all.vars]))
head(env.vars[,all.vars])
### 7. Reduce predictos by VIF ######
# viffer function on github
# dd = env.vars[,all.vars]
# #dd = env.vars[,all.vars[!all.vars %in% "be30"]]
# varrem = 'a'; maxvif = 100
# names(dd)
# while  (maxvif>=8 ) {
#   if (varrem!='a') { dd = dplyr::select(dd, -all_of(varrem)) }
#
#   vif = corvif(dd)
#   order = order(vif$GVIF, decreasing=F)
#   vif.count = data.frame(var=names(dd)[order], vif=vif[order,])
#   maxvif = max(vif.count$vif)
#   varrem = as.character(vif.count$var[ncol(dd)])
#
#   print(c(ncol(dd), varrem))
# }
# vif.vars <- vif.count$var
source("https://raw.githubusercontent.com/Cdevenish/R-Material/master/Functions/Eco/viffer.r")
vif <- viffer(env.vars[,all.vars], keep = c("be30"), z = 8)
vif <- viffer(env.vars[,all.vars], z = 8)
vif.vars <- rownames(vif)
vars <- c(vif.vars, "insideHJA")
varsName <- "vars11"
#
# rm(dd, varrem, vif, order)
rm(vif.vars, vif)
## separate env.vars into evaluation and non evaluation data sets
env.vars.test <- env.vars[env.vars$SiteName %in% test.Names, ]
env.vars <- env.vars[env.vars$SiteName %in% train.Names, ]
# varsName <- "vars6"
# vars <- c("ht30","gt4_r30","gt4_250",
#           "cut_r250","cut40_r1k","cut40_r250",
#           "be30", "slope30", "Nss30","Ess30","twi30","tpi250","tpi1k",
#           "l_Cover_2m_max", "l_Cover_4m_16m", "l_rumple",
#           "DistStream","DistRoad",
#           "ndmi_stdDev_r100", "ndmi_stdDev_r500","nbr_stdDev_r100", "nbr_stdDev_r500", "ndvi_p5_r100","ndvi_p5_r500",
#           "ndvi_p50_r100","ndvi_p50_r500", "ndmi_p50_r100", "ndmi_p50_r500",
#           "LC08_045029_20180726_B4","LC08_045029_20180726_B5","LC08_045029_20180726_B10",
#           "minT_annual","precipitation_mm")
# load VIF vars and hard code below
# load(file.path(resFolder, "selected-vars.rdata"))
# cat(paste(vif.count$var, collapse = '", "'))
# varsName <- "vars7" # VIF5
# vars7 <- c("gt4_r30", "gt4_500", "cut_40msk", "cut_r1k", "cut_r250", "cut40_r1k", "cut40_r250", "tri30", "Nss30", "Ess30", "twi30", "tpi250", "tpi1k", "l_Cover_2m_4m", "l_Cover_4m_16m", "l_p25", "l_rumple", "DistStream", "DistRoad", "insideHJA", "ndmi_stdDev_r100", "nbr_stdDev_r250", "ndvi_p5_r100", "ndvi_p95_r500", "ndmi_p95_r100", "ndmi_p95_r500", "LC08_045029_20180726_B3", "LC08_045029_20180726_B5", "LC08_045029_20180726_B10", "minT_annual")
# varsName <- "vars8" # be30 replaces minT
# vars <- c("Ess30", "DistRoad", "Nss30", "DistStream", "tri30", "LC08_045029_20180726_B5", "l_rumple", "cut_40msk", "insideHJA", "cut_msk", "cut40_r1k", "l_Cover_2m_4m", "l_Cover_4m_16m", "cut_r1k", "be30", "ndmi_p95_r100", "ndvi_p50_r500", "cut40_r250", "nbr_stdDev_r250", "precipitation_mm", "gt4_250", "tpi250", "cut_r250", "LC08_045029_20180726_B1", "ndvi_p50_r100", "gt4_r30", "twi30", "ndvi_p5_r100", "tpi1k", "ndvi_p5_r500", "l_p25", "LC08_045029_20180726_B10")
#
# vars <- c(
#   "ht30","gt4_r30","gt4_250",
#   "cut_r250", "cut_r1k", "cut40_r1k","cut40_r250",
#   "be30", "slope30", "Nss30","Ess30","twi30","tpi250","tpi1k",
#   "lg_cover2m_max", "lg_cover4m_16m", "l_rumple",
#   "lg_DistStream","lg_DistRoad",
#   "ndmi_stdDev_r100", "ndmi_stdDev_r500","nbr_stdDev_r100", "nbr_stdDev_r500",
#   "ndvi_p5_r100","ndvi_p5_r500",
#   "ndvi_p50_r100","ndvi_p50_r500", "ndmi_p50_r100", "ndmi_p50_r500", "ndmi_p95_r100", "ndmi_p95_r500",
#   "LC08_045029_20180726_B4","LC08_045029_20180726_B5","LC08_045029_20180726_B10")
#
# varsName <- "vars10"
# check names
all(vars %in% colnames(env.vars))
## Save model data
save(otu.pa.csv, otu.qp.csv, otu.pa.csv.test, otu.qp.csv.test, otuenv, env.vars, env.vars.test,
spChoose, test.Names, train.Names, select.percent,
k, minocc, noSteps, vars, varsName, abund, device, iter, sampling,
file = file.path(resFolder, paste0("modelData_",abund,".rdata")))
library(raster)
library(sf)
pt1 = st_point(c(0,1))
pt2 = st_point(c(1,1))
st_sfc(pt1, pt2)
d = data.frame(a = 1:2)
d$geom = st_sfc(pt1, pt2)
df = st_as_sf(d)
print(df)
plot(df)
plot(st_geometry(df))
#plot(st_geometry(df))
r_df <- rasterize(df)
#plot(st_geometry(df))
extent(df)
#plot(st_geometry(df))
raster(extent(df))
pt1 = st_point(c(0,1))
pt2 = st_point(c(1,2))
st_sfc(pt1, pt2)
d = data.frame(a = 1:2)
d$geom = st_sfc(pt1, pt2)
df = st_as_sf(d)
print(df)
#plot(st_geometry(df))
raster(extent(df))
r_df <- rasterize(df, raster(extent(df)))
plot(r_df)
r_df <- rasterize(df, raster(extent(df)), field = "a")
plot(r_df)
baseFolder <- "code_sjSDM/r20210716b"
resFolder <- file.path(baseFolder, "results")
plotsFolder <- file.path(baseFolder, "plots")
sppFolder <- file.path(resFolder, "spp_tifs")
dir.create(sppFolder)
if(dir.create(sppFolder)) print("yes")
sppFolder <- file.path(resFolder, "spp_tifs")
if(!dir.exists(sppFOlder)) dir.create(sppFolder)
if(!dir.exists(sppFolder)) dir.create(sppFolder)
if(!dir.exists(sppFolder)) dir.create(sppFolder)
devtools::install_github("https://github.com/TheoreticalEcology/s-jSDM", subdir = "sjSDM")
install.packages("devtools")
devtools::install_github("https://github.com/TheoreticalEcology/s-jSDM", subdir = "sjSDM")
devtools::install_github("https://github.com/TheoreticalEcology/s-jSDM", subdir = "sjSDM")
Sys.setenv(R_REMOTES_STANDALONE="true")
remotes::install_github("https://github.com/TheoreticalEcology/s-jSDM", subdir = "sjSDM")
sjSDM::install_sjSDM(version = "cpu")
y
load("D:/CD/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada/code_sjSDM/r20210716b/results/modelData_pa.rdata")
load("D:/CD/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada/data/newData_scaled_clamp.rdata")
newData_clamp_wide.sc <- newData_clamp_wide.sc %>%
dplyr::select(all_of(vars))
library(dplyr)
newData_clamp_wide.sc <- newData_clamp_wide.sc %>%
dplyr::select(all_of(vars))
setwd("D:/CD/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
resFolder <-"code_sjSDM/r20210716b/results"
## Load new data for prediction and new scaled data
load("data/newData_scaled.rdata") # # newData.sc, xy.sites.sc, allVars.sc (same as in clamp apart from newData.sc)
load("data/newData_scaled_clamp.rdata") #newData_clamp_wide.sc, xy.sites.sc, newXY.sc, allVars.sc,
load("D:/CD/UEA/Oregon/gis/processed_gis_data/r_oversize/newData_scaled.rdata")
newData.sc <- newData.sc %>%
dplyr::select(all_of(vars))
