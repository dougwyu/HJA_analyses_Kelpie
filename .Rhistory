date.model.run = 20210119   # !!! change accordingly
outputidxstatstabulatefolder = glue("outputs_minimap2_{minimaprundate}_{samtoolsfilter}_{samtoolsqual}_kelpie{kelpierundate}_{primer}_vsearch97")
outputpath = glue('../../Kelpie_maps/{outputidxstatstabulatefolder}')
library(glue)
library(dplyr)
outputidxstatstabulatefolder = glue("outputs_minimap2_{minimaprundate}_{samtoolsfilter}_{samtoolsqual}_kelpie{kelpierundate}_{primer}_vsearch97")
outputpath = glue('../../Kelpie_maps/{outputidxstatstabulatefolder}')
sjsdmV = '0.1.3.9000' # package version
# names for graph
sjsdmVfolder = glue('sjsdm-{sjsdmV}')
# ..... load data ......
alldata = read.csv(here(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')), header=T, sep=',', stringsAsFactors = F, na.strings='NA')
wd <- getwd()
wd
alldata = read.csv(here(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')), header=T, sep=',', stringsAsFactors = F, na.strings='NA')
outputpath = glue('Kelpie_maps/{outputidxstatstabulatefolder}')
alldata = read.csv(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv'), header=T, sep=',', stringsAsFactors = F, na.strings='NA')
file.path(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')
)
alldata = read.csv(file.path(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')), header=T, sep=',', stringsAsFactors = F, na.strings='NA')
dim(alldata)
names(alldata)[1:10]
load("Hmsc_CD/oregon_ada/data/demStats.rdata")
str(dem_stats)
trap; period
alldata1 = subset(alldata, trap == 'M1' & period == 'S1')
dim(alldata1)
names(alldata1)[102:103]
a = alldata1 %>% dplyr::select(contains('__'))
a = a[,which(specnumber(a, MARGIN=2)>=minocc)]
a = a[,which(vegan::specnumber(a, MARGIN=2)>=minocc)]
a = alldata1 %>% dplyr::select(contains('__'))
a = a[,which(vegan::specnumber(a, MARGIN=2)>=minocc)]
dim(a)
alldata1 = cbind(dplyr::select(alldata1, -contains('__')), a)
dim(alldata1)
num.sample = dim(a)[1]
select.percent = .8
ssdd = 100 		# please keep this value to make results comparable!
set.seed(ssdd)
a = base::sample(1:num.sample, round(num.sample*select.percent))
num.sample
num.sample*select.percent
otu = dplyr::select(alldata1, contains('__'))
otu.train = otu[a,]
dim(otu.train)
otu.test = otu[-a,]
envnames = c("insideHJA", "elevation_m", "canopyHeight_m", "minT_annual", "precipitation_mm", "distToRoad_m", "distToStream_m", "YrsSinceDist", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_Cover_2m_max", "l_Cover_2m_4m", "l_Cover_4m_16m", "l_p25", "l_p95", "l_rumple")
ori.env = dplyr::select(left_join(dplyr::select(alldata1, envnames, "SiteName"), dplyr::select(dem_stats, 'SiteName', 'tri.pt'), by=c('SiteName'='SiteName')), -'SiteName')
# add 'roughness' -> tri.pt
ori.env.train = ori.env[a,]
ori.env.test = ori.env[-a,]
str(ori.env.test)
ori.XY = dplyr::select(alldata1, starts_with('UTM'))
ori.XY.train = ori.XY[a,]
ori.XY.test = ori.XY[-a,]
str(ori.XY.train)
# ... view data ...
par(mfrow=c(1,2))
hist(ori.env.train$elevation_m)
hist(ori.env$elevation_m)
ggplot(ori.XY, aes(UTM_E, UTM_N)) + geom_point() + geom_point(data=ori.XY.train, aes(colour='red')) + scale_colour_manual(labels = c('training'), values = c("red"))
library(ggplot2)
ggplot(ori.XY, aes(UTM_E, UTM_N)) + geom_point() + geom_point(data=ori.XY.train, aes(colour='red')) + scale_colour_manual(labels = c('training'), values = c("red"))
if (abund == 'pa')
{
otu.train = as.data.frame((otu.train>0)*1)
otu.test = as.data.frame((otu.test>0)*1)
}
# .. env data
scale.env.train.all = dplyr::select(ori.env.train, -'insideHJA') %>% scale()
str(scale.env.train.all)
scale.env.train = data.frame(scale.env.train.all) %>% add_column(insideHJA=as.factor(ori.env.train$insideHJA), .before=names(ori.env.train)[2])
??add_column
scale.env.train = data.frame(scale.env.train.all) %>% tibble::add_column(insideHJA=as.factor(ori.env.train$insideHJA), .before=names(ori.env.train)[2])
str(scale.env.train)
dd.env.scaler = data.frame(t(data.frame(env.mean = attr(scale.env.train.all, "scaled:center"), env.sd = attr(scale.env.train.all, "scaled:scale"))))
str(dd.env.scaler)
rm(scale.env.train.all)
scale.env.test = as.data.frame(do.call(rbind, apply(dplyr::select(ori.env.test, -'insideHJA'), 1, function(x){(x-dd.env.scaler['env.mean',])/dd.env.scaler['env.sd',]} ) )) %>% add_column(insideHJA=as.factor(ori.env.test$insideHJA), .before=names(ori.env.test)[2])
scale.env.test = as.data.frame(do.call(rbind, apply(dplyr::select(ori.env.test, -'insideHJA'), 1, function(x){(x-dd.env.scaler['env.mean',])/dd.env.scaler['env.sd',]} ) )) %>% tibble::add_column(insideHJA=as.factor(ori.env.test$insideHJA), .before=names(ori.env.test)[2])
str(scale.env.test)
XY.train.all = scale(ori.XY.train)
str(XY.train.all)
XY.train = data.frame(XY.train.all)
str(XY.train)
dd.xy.scaler = data.frame(t(data.frame(sp.mean = attr(XY.train.all, "scaled:center"), sp.sd = attr(XY.train.all, "scaled:scale"))))
str(dd.xy.scaler)
base::rownames(dd.xy.scaler)
rm(XY.train.all)
XY.test = as.data.frame(do.call(rbind, apply(ori.XY.test, 1, function(x){(x-dd.xy.scaler['sp.mean',])/dd.xy.scaler['sp.sd',]} ) ))
str(XY.test)
# ... view data ...
par(mfrow=c(2,2))
hist(ori.env.train[,2],xlim=c(1000,5500), breaks = 10)
hist(ori.env.test[,2],xlim=c(1000,5500), breaks = 10)
hist(scale(ori.env.test[,2]))
hist(scale.env.test[,2])
par(mfrow=c(1,2))
hist(XY.test[,2])
hist(XY.train[,2])
rm(dd.env.scaler, dd.xy.scaler)
s.otu.train = as.matrix(otu.train)
attr(s.otu.train, 'dimnames') = NULL
str(s.otu.train)
# set variables
formula.env = 'envDNN'
lambda.env = .1
alpha.env = .5
lambda.sp = .1
alpha.sp = .9
hidden = c(50L,50L,10L)
acti.sp = 'relu'
drop = .3
save(s.otu.train,scale.env.train, XY.train,  s.otu.test, scale.env.test, XY.test, file = "Hmsc_CD/oregon_ada/data/yuanghen_mod_data.rdata")
s.otu.test = as.matrix(otu.test)
attr(s.otu.test, 'dimnames') = NULL
str(s.otu.test)
save(s.otu.train,scale.env.train, XY.train,  s.otu.test, scale.env.test, XY.test, file = "Hmsc_CD/oregon_ada/data/yuanghen_mod_data.rdata")
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
load("data/yuanghen_mod_data.rdata")
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
source("code_sjSDM/S1_read_data.r")
raretaxa <- which(colSums(Y.train.pa > 0) < 10)
length(raretaxa)
Y.train.pa_min10 <- as.matrix(Y.train.pa[, -raretaxa]) # reduced species
rm(raretaxa)
XFormula1 <- as.formula(~be10+B11_median+mean.EVI+insideHJA + Ess + ht + ht.r500 + cov4_16 + cov4_16.r500 + mTopo)
# check names
all(all.vars(XFormula1) %in% names(X.train))
str(X.train)
raretaxa <- which(colSums(Y.train.pa > 0) < 10)
length(raretaxa)
Y.train.pa_min10 <- as.matrix(Y.train.pa[, -raretaxa]) # reduced species
rm(raretaxa)
XFormula1 <- as.formula(~be10+B11_median+mean.EVI+insideHJA + Ess + ht + ht.r500 + cov4_16 + cov4_16.r500 + mTopo)
# check names
all(all.vars(XFormula1) %in% names(X.train))
str(X.train)
xy.scale <- X.train[,c("UTM_N", "UTM_E")]
str(X.train)
head(S.train)
xy.scale <- scale(S.train[,c("UTM_E", "UTM_N")])
head(xy.scale)
seq(0,0.1,0.001)
seq(0, 1, 0.1)
seq(0, 1, 0.2)
seq(0, 0.1, 0.001)
seq(0, 0.1, 0.01)
load("Hmsc_CD/oregon_ada/results_sjSDM/oregon_trial.rdata")
getwd()
load("results_sjSDM/oregon_trial.rdata")
getwd()
load("results_sjSDM/oregon_trial.rdata")
load("Hmsc_CD/oregon_ada/results_sjSDM/oregon_trial.rdata")
head(link_pred)
summary(link_pred)
family(link="probit")
binomial(link="probit")
binomial(link="probit")$linkinv
n <- 1000
beta0 <- -1.6
beta1 <- 0.03
x <- runif(n=n, min=18, max=60)
pi_x <- exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x))
y <- rbinom(n=length(x), size=1, prob=pi_x)
data <- data.frame(x, pi_x, y)
names(data) <- c("age", "pi", "y")
print(data)
glm2 <- glm(y ~ x, family = binomial)
summary(glm2)
pred_link_scale <- predict(glm2, type = "link")
n <- 100
beta0 <- -1.6
beta1 <- 0.03
x <- runif(n=n, min=18, max=60)
pi_x <- exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x))
y <- rbinom(n=length(x), size=1, prob=pi_x)
data <- data.frame(x, pi_x, y)
names(data) <- c("age", "pi", "y")
print(data)
head(data)
glm2 <- glm(y ~ x, family = binomial)
summary(glm2)
pred_link_scale <- predict(glm2, type = "link")
head(pred_link_scale)
pred_resp_scale <- predict(glm2, type = "response")
head(pred_resp_scale)
binomial(link="probit")$linkinv
binomial(link="probit")$linkinv(pred_link_scale)
pred_transformed <- binomial(link="probit")$linkinv(pred_link_scale)
pred_transformed == pred_resp_scale
binomial(link="probit")$linkinv
glm2 <- glm(y ~ x, family = binomial(link = "probit"))
summary(glm2)
pred_link_scale <- predict(glm2, type = "link")
head(pred_link_scale)
pred_resp_scale <- predict(glm2, type = "response")
head(pred_resp_scale)
pred_transformed <- binomial(link="probit")$linkinv(pred_link_scale)
head(pred_transformed)
pred_transformed == pred_resp_scale
all(pred_transformed == pred_resp_scale)
head(link_pred)
rm(b, beta0, beta1, x, pi_x, y, data, glm2, pred_link_scale, pred_resp_scale, pred_transformed)
rm(n, b, beta0, beta1, x, pi_x, y, data, glm2, pred_link_scale, pred_resp_scale, pred_transformed)
head(link_pred)
summary(link_pred)
pred_resp <- binomial(link="probit")$linkinv(link_pred)
head(link_pred)
link_pred[1:10,1:10]
(binomial(link = "probit")$linkinv(raw_pred))[1:10,1:10]
summary(raw_pred)
(binomial(link = "probit")$linkinv(raw_pred))[1:10,1:10]
link_pred[1:10,1:10]
(binomial(link = "logit")$linkinv(raw_pred))[1:10,1:10]
link_pred[1:10,1:10]
(binomial(link = "probit")$linkinv(raw_pred))[1:10,1:10]
source("code_sjSDM/S1_read_data.r")
here::here()
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
setwd(wd)
wd <- here::here()
setwd(wd)
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
source("code_sjSDM/S1_read_data.r")
rm(wd)
raretaxa <- which(colSums(Y.train.pa > 0) < 10)
length(raretaxa)
Y.train.pa_min10 <- as.matrix(Y.train.pa[, -raretaxa]) # reduced species
rm(raretaxa)
m1 <- matrix(c(1,5,3,7,3,4,5,5,7,8,8,10), ncol = 3)
m1
m2 <- matrix(c(2,0,3,0,3,4,10,5,10,5,7,1), ncol = 3)
m1 * m2
m1
m2
m1 * m2
m1 <- matrix(c(1,5,3,7,3,4,5,5,7,8,8,10), ncol = 3)
m2 <- matrix(c(2,0,3,0,3,4,10,5,10,5,7,10), ncol = 3)
m1
m2
m1 * m2
prob.cm <- function(pred_data, obs_data){
TP <- sum(pred_data * obs_data)
FP <- sum(pred_data * (1 - obs_data))
TN <- sum((1 - pred_data) * (1 - obs_data))
FN <- sum((1 - pred_data) * obs_data)
print(matrix(TP, FP, TN, FN), ncol = 2)
return(list(TP, FP, TN, FN))
}
cm <- prob.cm(link_pred, Y.train.pa_min10)
m1 * m2
sum(m1 * m2)
colSums(m1 * m2)
prob.cm <- function(pred_data, obs_data){
TP <- colSums(pred_data * obs_data)
FP <- colSums(pred_data * (1 - obs_data))
TN <- colSums((1 - pred_data) * (1 - obs_data))
FN <- colSums((1 - pred_data) * obs_data)
return(list(TP, FP, TN, FN))
}
cm <- prob.cm(link_pred, Y.train.pa_min10)
prob.cm <- function(pred_data, obs_data){
TP <- colSums(pred_data * obs_data)
FP <- colSums(pred_data * (1 - obs_data))
TN <- colSums((1 - pred_data) * (1 - obs_data))
FN <- colSums((1 - pred_data) * obs_data)
return(cbind(TP=TP, FP=FP, TN=TN, FN=FN))
}
cm <- prob.cm(link_pred, Y.train.pa_min10)
haed(cm)
head(cm)
cm.bin <- prob.cm(link_pred>0, Y.train.pa_min10)
head(cm.bin)
colSums(link_pred>0)
load("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada/results_sjSDM/oregon_trial_tune.rdata")
str(model, max.level = 1)
str(tune_results, max.level = 1)
tune_results$short_summary
head(tune_results$short_summary)
str(tune_results, max.level = 1)
str(tune_results$tune_results, max.level = 1)
str(tune_results$tune_results[[1]], max.level = 1)
str(tune_results$tune_results[[1]][[1]], max.level = 1)
str(tune_results, max.level = 1)
haed(tune_results$summary)
head(tune_results$summary)
str(tune_results, max.level = 1)
summary(tune_results$summary
)
60*5
head(tune_results$summary) # here 60 iterations, times 5 for cross validation = 60*5
str(tune_results, max.level = 1)
tune_results$settings
str(tune_results, max.level = 1)
str(tune_results$tune_results[[1]][[1]], max.level = 1) # cross validation results
str(tune_results, max.level = 1)
str(tune_results$tune_results[[1]], max.level = 1) # cross validation results
str(tune_results$tune_results[[1]][[1]], max.level = 1) # cross validation results
max(tune_results$short_summary$AUC_test)
summary(tune_results)
best
wd <- here::here() # should have root at ".../HJA_analyses_Kelpie"
wd
setwd(wd)
dir()
# wd set on pipeline on Ada
library(dplyr)
# library(here) # not on ADA
library(glue)
samtoolsfilter <- "F2308" # F2308 filter only
samtoolsqual <- "q48"
minimaprundate <- 20200929
kelpierundate <- 20200927
primer <- "BF3BR2"
# reading from github online enables this to be used on ADA
gitHub <- "https://raw.githubusercontent.com/dougwyu/HJA_analyses_Kelpie/master/Kelpie_maps"
outputidxstatstabulatefolder <- glue::glue("outputs_minimap2_{minimaprundate}_{samtoolsfilter}_{samtoolsqual}_kelpie{kelpierundate}_{primer}_vsearch97")
datFile <- glue("sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_uncorr.csv")
otuenv <- read.csv(file.path(gitHub, outputidxstatstabulatefolder, datFile))
# M1S1
trap <- "M1"
period <- "S1"
otuenv <- otuenv %>%
dplyr::filter(trap == trap[[1]] & period == period[[1]])
# clean up
rm(datFile, gitHub, kelpierundate, minimaprundate, outputidxstatstabulatefolder, period, primer, samtoolsfilter, samtoolsqual, trap)
# bring in DEM stats
# load("data/demStats.rdata") # temporary location for moment...  REPLACED By topo.df
# load new topo vars
## load("data/topo_data.rdata")# change route if on ADA
load("Hmsc_CD/oregon_ada/data/topo_data.rdata")
# head(topo.df)
# remove NA
# sum(is.na(topo.df))
# ind <- which(is.na(topo.df$twi))
# topo.df <- topo.df[-ind,]
#
# # same with others
# otuenv <- otuenv[-ind,]
# rm(ind)
# keep OTUs with >=5 incidences
# original read number abundance
minocc <- 5 # set to high number (e.g. 20) for testing
otu.ab.csv <- otuenv %>% dplyr::select(contains("__"))
otu.ab.csv <- otu.ab.csv[ , colSums(otu.ab.csv > 0) >= minocc]
# log(FSL) correction and scale to quasiprobability
otu.qp.csv <- otu.ab.csv %>%
mutate(across(contains("__"),
~ .x /(otuenv$COISpike_sum*otuenv$lysis_ratio))) %>%
mutate(across(contains("__"), ~ log(.x + 0.001))) %>%
mutate(across(contains("__"), ~ scales::rescale(.x))) # {scales}
max(otu.qp.csv) == 1 # should be TRUE
# otu.qp.csv[1:10, 1:5]
# convert to presence/absence data
otu.pa.csv <- otu.ab.csv
otu.pa.csv[otu.pa.csv > 0] <- 1
min(colSums(otu.pa.csv)) == minocc # should be TRUE
Y.train.pa <- otu.pa.csv
Y.train.qp <- otu.qp.csv
rm(minocc)
# env covariates
# otuenv %>%
#   dplyr::select(!contains("__"), -UTM_E, -UTM_N, -starts_with("nor")) %>%
#   names(.)
#  [1] "SiteName"           "trap"               "period"
#  [4] "lysis_ratio"        "COISpike_sum"       "clearcut"
#  [7] "insideHJA"          "oldGrowthIndex"     "elevation_m"
# [10] "canopyHeight_m"     "minT_annual"        "maxT_annual"
# [13] "precipitation_mm"   "distToRoad_m"       "distToStream_m"
# [16] "YrsSinceDist"       "B1_20180717"        "B2_20180717"
# [19] "B3_20180717"        "B4_20180717"        "B5_20180717"
# [22] "B6_20180717"        "B7_20180717"        "B10_20180717"
# [25] "B11_20180717"       "NDVI_20180717"      "EVI_20180717"
# [28] "B_20180717"         "G_20180717"         "W_20180717"
# [31] "B1_20180726"        "B2_20180726"        "B3_20180726"
# [34] "B4_20180726"        "B5_20180726"        "B6_20180726"
# [37] "B7_20180726"        "B10_20180726"       "B11_20180726"
# [40] "NDVI_20180726"      "EVI_20180726"       "B_20180726"
# [43] "G_20180726"         "W_20180726"         "B1_20180802"
# [46] "B2_20180802"        "B3_20180802"        "B4_20180802"
# [49] "B5_20180802"        "B6_20180802"        "B7_20180802"
# [52] "B10_20180802"       "B11_20180802"       "NDVI_20180802"
# [55] "EVI_20180802"       "B_20180802"         "G_20180802"
# [58] "W_20180802"         "B1_20180818"        "B2_20180818"
# [61] "B3_20180818"        "B4_20180818"        "B5_20180818"
# [64] "B6_20180818"        "B7_20180818"        "B10_20180818"
# [67] "B11_20180818"       "NDVI_20180818"      "EVI_20180818"
# [70] "B_20180818"         "G_20180818"         "W_20180818"
# [73] "mean.NDVI"          "mean.EVI"           "mean.bright"
# [76] "mean.green"         "mean.wet"           "mean.NDVI.scale"
# [79] "mean.EVI.scale"     "mean.green.scale"   "mean.bright.scale"
# [82] "mean.wet.scale"     "l_Cover_2m_max"     "l_Cover_2m_max_all"
# [85] "l_Cover_2m_4m"      "l_Cover_2m_4m_all"  "l_Cover_4m_16m"
# [88] "l_p25"              "l_p25_all"          "l_p95"
# [91] "l_p95_all"          "l_rumple"
# how many otu in total?
# sum(grepl("__", colnames(otuenv)))
# remove OTUs, XY, and normalised NDVI and EVI
# average, optionally log, select, and scale env covariates
env.vars <- otuenv %>%
dplyr::select(!contains("__"), -UTM_E, -UTM_N, -starts_with("nor")) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"),
elevation_m = elevation_m * 0.3048, ## convert to metres???
B1_median = apply(across(starts_with("B1_")), 1, median),
B2_median = apply(across(starts_with("B2_")), 1, median),
B3_median = apply(across(starts_with("B3_")), 1, median),
B4_median = apply(across(starts_with("B4_")), 1, median),
B5_median = apply(across(starts_with("B5_")), 1, median),
B6_median = apply(across(starts_with("B6_")), 1, median),
B7_median = apply(across(starts_with("B7_")), 1, median),
B10_median = apply(across(starts_with("B10_")), 1, median),
B11_median = apply(across(starts_with("B11_")), 1, median),
lg_DistStream = log(distToStream_m + 0.001),
lg_DistRoad = log(distToRoad_m + 0.001),
lg_YrsDisturb = log(YrsSinceDist + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001)) %>%
dplyr::select(uniqueID, clearcut,insideHJA,oldGrowthIndex, elevation_m, canopyHeight_m, precipitation_mm, minT_annual, maxT_annual, mean.NDVI, mean.EVI, mean.green, mean.wet, mean.bright, l_p25, l_rumple, B1_median, B2_median,B3_median,B4_median,B5_median,B6_median,B7_median,B10_median,B11_median,lg_DistStream, lg_DistRoad, lg_YrsDisturb, lg_cover2m_max, lg_cover2m_4m, lg_cover4m_16m) %>%
dplyr::left_join(y = topo.df, by = "uniqueID") %>%
mutate(
#across(where(is.numeric), scale), # scale here
clearcut = factor(clearcut),
insideHJA = factor(insideHJA))
X.train <- env.vars
head(env.vars)
summary(env.vars)
env.vars <- otuenv %>%
dplyr::select(!contains("__"), -UTM_E, -UTM_N, -starts_with("nor")) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"),
elevation_m = elevation_m * 0.3048, ## convert to metres???
B1_median = apply(across(starts_with("B1_")), 1, median),
B2_median = apply(across(starts_with("B2_")), 1, median),
B3_median = apply(across(starts_with("B3_")), 1, median),
B4_median = apply(across(starts_with("B4_")), 1, median),
B5_median = apply(across(starts_with("B5_")), 1, median),
B6_median = apply(across(starts_with("B6_")), 1, median),
B7_median = apply(across(starts_with("B7_")), 1, median),
B10_median = apply(across(starts_with("B10_")), 1, median),
B11_median = apply(across(starts_with("B11_")), 1, median),
lg_DistStream = log(distToStream_m + 0.001),
lg_DistRoad = log(distToRoad_m + 0.001),
lg_YrsDisturb = log(YrsSinceDist + 0.001),
lg_cover2m_max = log(l_Cover_2m_max + 0.001),
lg_cover2m_4m = log(l_Cover_2m_4m + 0.001),
lg_cover4m_16m = log(l_Cover_4m_16m + 0.001)) %>%
dplyr::select(uniqueID, clearcut,insideHJA,oldGrowthIndex, elevation_m, canopyHeight_m, precipitation_mm, minT_annual, maxT_annual, mean.NDVI, mean.EVI, mean.green, mean.wet, mean.bright, l_p25, l_p95, l_rumple, B1_median, B2_median,B3_median,B4_median,B5_median,B6_median,B7_median,B10_median,B11_median,lg_DistStream, lg_DistRoad, lg_YrsDisturb, lg_cover2m_max, lg_cover2m_4m, lg_cover4m_16m) %>%
dplyr::left_join(y = topo.df, by = "uniqueID") %>%
mutate(
#across(where(is.numeric), scale), # scale here # scale when defining models etc.
clearcut = factor(clearcut),
insideHJA = factor(insideHJA))
summary(env.vars)
X.train <- env.vars
## Study design data
S.train <- otuenv %>%
dplyr::select(SiteName,trap,period, UTM_E, UTM_N) %>%
mutate(uniqueID = paste(SiteName, trap, period, sep = "_"))
head(S.train)
spp <- data.frame(species = colnames(Y.train.pa)) %>%
tidyr::separate(col = species, into = c("OTU", "empty", "class", "order", "family",
"genus", "epithet", "BOLD", "BOLDID",
"size"),
remove = FALSE, sep = "_") %>%
select(-empty)
head(spp)
# convert to NAs
for(c in seq_along(spp)[-1]) spp[,c] <- sub("NA", NA, spp[,c])
# Add dummy family and genus
spp$family[is.na(spp$family)] <- sprintf("fam%03d", 1:sum((is.na(spp$family))))
spp$genus[is.na(spp$genus)] <- sprintf("gen%03d", 1:sum((is.na(spp$genus))))
head(spp)
# convert to factors for ape
spp <- spp[order(spp$class, spp$order, spp$family, spp$genus),]
tax.cols <- c("class", "order", "family", "genus", "epithet", "species")
for(i in tax.cols) spp[,i] <- factor(spp[,i])
head(spp)
P <- ape::as.phylo(~class/order/family/genus/species, data = spp, collapse = F)
P$edge.length = rep(1, length(P$edge)) # make all lengths eqaul between tax levels
ape::is.rooted(P)
all(P$tip.label %in% colnames(Y.train.pa))
all(P$tip.label %in% colnames(Y.train.qp))
# save(Y.train.pa, Y.train.qp, X.train, S.train, P, file = "data/allData_vif.rdata")
rm(c, i, tax.cols, spp)
# rm(otu.ab.csv, otuenv)
write.csv(env.vars, "HJA_scripts/10_eo_data/biodiversity_site_info_GIS_20210127.csv", row.names = F)
write.csv(env.vars, "Hmsc_CD/local/data/biodiversity_site_info_GIS_20210127.csv", row.names = F)
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
abund = 'qp'		# 'qp','pa'
file.path("data", "otu.train.", abund, ".csv")
paste0("data", "otu.train.", abund, ".csv")
paste0("data/otu.train.", abund, ".csv")
scale.env.train <- read.csv("data/scale.env.train.csv")
s.otu.train <- read.csv(paste0("data/otu.train.", abund, ".csv"))
XY.train <- read.csv("data/scale.XY.train.csv")
s.otu.train <- read.csv(paste0("data/otu.train.", abund, ".csv"))
scale.env.train <- read.csv("data/scale.env.train.csv")
XY.train <- read.csv("data/scale.XY.train.csv")
s.otu.train = as.matrix(otu.train)
otu.train <- read.csv(paste0("data/otu.train.", abund, ".csv"))
s.otu.train = as.matrix(otu.train)
attr(s.otu.train, 'dimnames') = NULL
str(s.otu.train)
names(scale.env.train)
otu.train <- read.table(paste0("data/otu.train.", abund, ".csv"), sep = " ")
scale.env.train <- read.table("data/scale.env.train.csv", sep = " ")
str(s.otu.train)
head(scale.env.train)
scale.env.train <- read.table("data/scale.env.train.csv", sep = " ", header = T)
head(scale.env.train)
