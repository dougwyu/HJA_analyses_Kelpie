ls(envir = Env.lin2)
str(Env.lin2)
attr(Env.lin2$formula, ".Environment")
ls(envir = attr(Env.lin2$formula, ".Environment"))
library(sf)
options(sf_max.plot=1)
library(raster)
setwd("C:/Users/55116479/Dropbox (Manchester Met)/R_online/dbTapir")
load("pts_final.rdata")
set.seed(99)
bg <- dismo::randomPoints(stck.msk, 7500)
tmp <- extract(stck.msk[[1]], bg)
nrow(bg) - sum(complete.cases(tmp))
k <- 5
group <- dismo::kfold(st_coordinates(pts.p), k)
train <- sf::st_coordinates(pts.p)[group != i,]
test <- sf::st_coordinates(pts.p)[group == i,]
i = 1
train <- sf::st_coordinates(pts.p)[group != i,]
test <- sf::st_coordinates(pts.p)[group == i,]
mx <- dismo::maxent(x = stck.msk, p = train, a = bg)
str(mx)
predVals.pres <- predict(mx, mx@presence)
predVals.abs <- predict(mx, mx@absence)
p1 = mean(predVals.pres) # Mean predicted at pres.
p0 = mean(predVals.abs) # Mean predicted at background
d = p1-p0 # Calculate d for a given model
d
eval <- dismo::evaluate(p=test, a=bg, x=stck.msk, model = mx)
eval
str(eval)
eval@tjur <- d
evList <- as.list(eval)
head(pts.p)
sight.ab.av  <- rbind(st_coordinates(pts.p), bg)
sight.ab.av$pa <- c(rep(1, nrow(pts.p), rep(0, length(bg))))
sight.ab.av <- rbind(st_coordinates(pts.p), bg)
sight.ab.av$pa <- c(rep(1, nrow(pts.p)), rep(0, length(bg)))
nrow(pts.p)
length(bg)
sight.ab.av <- rbind(st_coordinates(pts.p), bg)
bg
st_coordinates(pts.p)
head(bg)
sight.ab.av <- rbind(st_coordinates(pts.p), bg)
sight.ab.av$pa <- c(rep(1, nrow(pts.p)), rep(0, nrow(bg)))
head(sight.ab.av)
nrow(pts.p)
nrow(bg)
sight.ab.av <- rbind(st_coordinates(pts.p), bg)
sight.ab.av <- data.frame(rbind(st_coordinates(pts.p), bg))
sight.ab.av$pa <- c(rep(1, nrow(pts.p)), rep(0, nrow(bg)))
head(sight.ab.av)
k <- 5
set.seed(99)
kfold <- dismo::kfold(sight.ab.av, k = k, by = pa)
kfold <- dismo::kfold(sight.ab.av, k = k, by = sight.ab.av$pa)
sight.ab.av <- data.frame(rbind(st_coordinates(pts.p), bg))
pa <- c(rep(1, nrow(pts.p)), rep(0, nrow(bg)))
head(sight.ab.av)
k <- 5
set.seed(99)
kfold <- dismo::kfold(sight.ab.av, k = k, by = pa)
table(kfold, pa)
mxRes <- list()
evRes <- list()
j = 1
trainData <- sight.ab.av[kfold != j,]
testData <- sight.ab.av[kfold == j,]
trainPA <- pa[kfold != j]
testPA <- pa[kfold == j]
mx <- dismo::maxent(x = trainData, p = trainPA, path = mxPath, removeDuplicates = TRUE)
mx <- dismo::maxent(x = trainData, p = trainPA, removeDuplicates = TRUE)
mxRes[[j]] <- mx
ev <- dismo::evaluate(p=testData[testPA == 1, ], a=testData[testPA == 0, ], model = mx)
ev
evRes[[j]] <- ev
predVals.pres <- predict(mx, mx@presence)
predVals.abs <- predict(mx, mx@absence)
p1 = mean(predVals.pres) # Mean predicted at pres.
p0 = mean(predVals.abs) # Mean predicted at background
d = p1-p0 # Calculate d for a given model
d
tjRes <- list()
for(j in 1:k){
# make training and test data
trainData <- sight.ab.av[kfold != j,]
testData <- sight.ab.av[kfold == j,]
# make training and test presence-absence info
trainPA <- pa[kfold != j]
testPA <- pa[kfold == j]
mx <- dismo::maxent(x = trainData, p = trainPA, removeDuplicates = TRUE)
mxRes[[j]] <- mx
#'evaluate'
ev <- dismo::evaluate(p=testData[testPA == 1, ], a=testData[testPA == 0, ], model = mx)
evRes[[j]] <- ev
predVals.pres <- predict(mx, mx@presence)
predVals.abs <- predict(mx, mx@absence)
p1 = mean(predVals.pres) # Mean predicted at pres.
p0 = mean(predVals.abs) # Mean predicted at background
d = p1-p0 # Calculate d for a given model
tjRes[[j]] <- d
# list(auc = eval@auc, tjur = d)
}
M2AUC<-sapply(evRes, function(x) mean(x@auc))
M2AUC
M2AUC <- sapply(evRes, function(x) x@auc)
M2AUC
sapply(tjRes, mean)
tjRes
unlist(tjRes)
mean(unlist(tjRes))
M2AUC <- sapply(evRes, function(x) x@auc)
M2AUC
getwd()
wd <- here::here()
setwd(wd)
dir()
rm(wd)
library(sf)
library(raster)
utm10N <- 32610
# EPSG:26910  NAD83 / UTM zone 10N
nadutm10 <- 26910
gis <- "J:/UEA/Oregon/gis"
dir(gis)
cov2_4 <- raster(file.path(gis, "r_utm", "lidar_metric_mosaic_Cover_2m_4m.tif"))
cov2_4
cov4_16 <- raster(file.path(gis, "r_utm", "lidar_metric_mosaic_Cover_4m_16m.tif"))
ht <- raster(file.path(gis, "r_utm", "lidar_metric_mosaic_p95.tif"))
ht
cut <- st_read(file.path(gis, "marie/disturbance.shp"))
cut
getwd()
wd <- here::here()
setwd(wd)
dir()
rm(wd)
library(sf)
library(raster)
# wgs84 UTM 10N
utm10N <- 32610
# EPSG:26910  NAD83 / UTM zone 10N
nadutm10 <- 26910
# EPSG:4269 # NAD 83
# nad83 <- 4269
gis <- "J:/UEA/Oregon/gis"
##
dir(gis)
# get points
# get data
source("Hmsc_CD/local/L1_read_data.r")
head(S.train)
xy.sf <- st_as_sf(S.train, coords = c("UTM_E", "UTM_N"), crs = nadutm10)
rm(S.train)
# transform to wgs utm to match rasters
xy.utm <- st_transform(xy.sf, crs = utm10N)
rm(xy.sf)
cut <- st_read(file.path(gis, "s_nad_utm/disturbance.shp"))
cut
cut[order(cut$YEAR, na.last = F),] # 0 shown in arcgis come in as NAs...
sum(is.na(cut$YEAR))
cut.utm <- st_transform(cut, crs = utm10N)
cut.utm
plot(cut.utm[, c("YEAR")])
plot(st_geometry(xy.utm), add = T, pch = 2, col = "black", cex = 1.5)
list.files(file.path(gis,"r_utm"), "lidar")
cov2_4 <- raster(file.path(gis, "r_utm", "lidar_metric_mosaic_Cover_2m_4m.tif"))
gis <- file.path(wd, "HJA_scripts/10_eo_data/raw_gis_data")
dir(gis)
gis <- file.path(wd, "HJA_scripts/10_eo_data/raw_gis_data") # pending some layers still, eg bareEarth.tif
getwd()
wd <- here::here()
setwd(wd)
dir()
gis <- file.path(wd, "HJA_scripts/10_eo_data/raw_gis_data") # pending some layers still, eg bareEarth.tif
dir(gis)
cut <- st_read(file.path(gis, "s_nad_utm/disturbance.shp"))
cut
cut[order(cut$YEAR, na.last = F),] # 0 shown in arcgis come in as NAs...
sum(is.na(cut$YEAR))
cut.utm <- st_transform(cut, crs = utm10N)
cut.utm
plot(cut.utm[, c("YEAR")])
plot(st_geometry(xy.utm), add = T, pch = 2, col = "black", cex = 1.5)
rm(cut)
list.files(file.path(gis,"r_utm"), "lidar")
cov2_4 <- raster(file.path(gis, "r_utm", "lidar_metric_mosaic_Cover_2m_4m.tif"))
cov4_16 <- raster(file.path(gis, "r_utm", "lidar_metric_mosaic_Cover_4m_16m.tif"))
ht <- raster(file.path(gis, "r_utm", "lidar_metric_mosaic_p95.tif"))
cov2_4
cov4_16
ht
hist(ht)
load("results_sjSDM/oregon_trial.rdata")
load("Hmsc_CD/oregon_ada/results_sjSDM/oregon_trial.rdata")
getwd()
load("Hmsc_CD/oregon_ada/results_sjSDM/oregon_trial.rdata")
str(model)
str(model, max.level = 1)
getwd() # should be /gpfs/home/hsp20azu/oregon_ada
samtoolsfilter = "F2308" # F2308 filter only
samtoolsqual = "q48"
minimaprundate = 20200929
kelpierundate = 20200927
primer = "BF3BR2"
minocc = 5
abund = 'qp' # pa , qp    # !!! change accordingly
trap <- "M1"; period = "S1"
date.model.run = 20210119   # !!! change accordingly
outputidxstatstabulatefolder = glue("outputs_minimap2_{minimaprundate}_{samtoolsfilter}_{samtoolsqual}_kelpie{kelpierundate}_{primer}_vsearch97")
outputpath = glue('../../Kelpie_maps/{outputidxstatstabulatefolder}')
library(glue)
library(dplyr)
outputidxstatstabulatefolder = glue("outputs_minimap2_{minimaprundate}_{samtoolsfilter}_{samtoolsqual}_kelpie{kelpierundate}_{primer}_vsearch97")
outputpath = glue('../../Kelpie_maps/{outputidxstatstabulatefolder}')
sjsdmV = '0.1.3.9000' # package version
# names for graph
sjsdmVfolder = glue('sjsdm-{sjsdmV}')
# ..... load data ......
alldata = read.csv(here(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')), header=T, sep=',', stringsAsFactors = F, na.strings='NA')
wd <- getwd()
wd
alldata = read.csv(here(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')), header=T, sep=',', stringsAsFactors = F, na.strings='NA')
outputpath = glue('Kelpie_maps/{outputidxstatstabulatefolder}')
alldata = read.csv(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv'), header=T, sep=',', stringsAsFactors = F, na.strings='NA')
file.path(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')
)
alldata = read.csv(file.path(outputpath, glue('sample_by_species_table_{samtoolsfilter}_minimap2_{minimaprundate}_kelpie{kelpierundate}_FSL_qp.csv')), header=T, sep=',', stringsAsFactors = F, na.strings='NA')
dim(alldata)
names(alldata)[1:10]
load("Hmsc_CD/oregon_ada/data/demStats.rdata")
str(dem_stats)
trap; period
alldata1 = subset(alldata, trap == 'M1' & period == 'S1')
dim(alldata1)
names(alldata1)[102:103]
a = alldata1 %>% dplyr::select(contains('__'))
a = a[,which(specnumber(a, MARGIN=2)>=minocc)]
a = a[,which(vegan::specnumber(a, MARGIN=2)>=minocc)]
a = alldata1 %>% dplyr::select(contains('__'))
a = a[,which(vegan::specnumber(a, MARGIN=2)>=minocc)]
dim(a)
alldata1 = cbind(dplyr::select(alldata1, -contains('__')), a)
dim(alldata1)
num.sample = dim(a)[1]
select.percent = .8
ssdd = 100 		# please keep this value to make results comparable!
set.seed(ssdd)
a = base::sample(1:num.sample, round(num.sample*select.percent))
num.sample
num.sample*select.percent
otu = dplyr::select(alldata1, contains('__'))
otu.train = otu[a,]
dim(otu.train)
otu.test = otu[-a,]
envnames = c("insideHJA", "elevation_m", "canopyHeight_m", "minT_annual", "precipitation_mm", "distToRoad_m", "distToStream_m", "YrsSinceDist", "B1_20180717", "B2_20180717", "B3_20180717", "B4_20180717", "B5_20180717", "B6_20180717", "B7_20180717", "B10_20180717", "B11_20180717", "NDVI_20180717", "EVI_20180717", "B_20180717", "G_20180717", "W_20180717", "l_Cover_2m_max", "l_Cover_2m_4m", "l_Cover_4m_16m", "l_p25", "l_p95", "l_rumple")
ori.env = dplyr::select(left_join(dplyr::select(alldata1, envnames, "SiteName"), dplyr::select(dem_stats, 'SiteName', 'tri.pt'), by=c('SiteName'='SiteName')), -'SiteName')
# add 'roughness' -> tri.pt
ori.env.train = ori.env[a,]
ori.env.test = ori.env[-a,]
str(ori.env.test)
ori.XY = dplyr::select(alldata1, starts_with('UTM'))
ori.XY.train = ori.XY[a,]
ori.XY.test = ori.XY[-a,]
str(ori.XY.train)
# ... view data ...
par(mfrow=c(1,2))
hist(ori.env.train$elevation_m)
hist(ori.env$elevation_m)
ggplot(ori.XY, aes(UTM_E, UTM_N)) + geom_point() + geom_point(data=ori.XY.train, aes(colour='red')) + scale_colour_manual(labels = c('training'), values = c("red"))
library(ggplot2)
ggplot(ori.XY, aes(UTM_E, UTM_N)) + geom_point() + geom_point(data=ori.XY.train, aes(colour='red')) + scale_colour_manual(labels = c('training'), values = c("red"))
if (abund == 'pa')
{
otu.train = as.data.frame((otu.train>0)*1)
otu.test = as.data.frame((otu.test>0)*1)
}
# .. env data
scale.env.train.all = dplyr::select(ori.env.train, -'insideHJA') %>% scale()
str(scale.env.train.all)
scale.env.train = data.frame(scale.env.train.all) %>% add_column(insideHJA=as.factor(ori.env.train$insideHJA), .before=names(ori.env.train)[2])
??add_column
scale.env.train = data.frame(scale.env.train.all) %>% tibble::add_column(insideHJA=as.factor(ori.env.train$insideHJA), .before=names(ori.env.train)[2])
str(scale.env.train)
dd.env.scaler = data.frame(t(data.frame(env.mean = attr(scale.env.train.all, "scaled:center"), env.sd = attr(scale.env.train.all, "scaled:scale"))))
str(dd.env.scaler)
rm(scale.env.train.all)
scale.env.test = as.data.frame(do.call(rbind, apply(dplyr::select(ori.env.test, -'insideHJA'), 1, function(x){(x-dd.env.scaler['env.mean',])/dd.env.scaler['env.sd',]} ) )) %>% add_column(insideHJA=as.factor(ori.env.test$insideHJA), .before=names(ori.env.test)[2])
scale.env.test = as.data.frame(do.call(rbind, apply(dplyr::select(ori.env.test, -'insideHJA'), 1, function(x){(x-dd.env.scaler['env.mean',])/dd.env.scaler['env.sd',]} ) )) %>% tibble::add_column(insideHJA=as.factor(ori.env.test$insideHJA), .before=names(ori.env.test)[2])
str(scale.env.test)
XY.train.all = scale(ori.XY.train)
str(XY.train.all)
XY.train = data.frame(XY.train.all)
str(XY.train)
dd.xy.scaler = data.frame(t(data.frame(sp.mean = attr(XY.train.all, "scaled:center"), sp.sd = attr(XY.train.all, "scaled:scale"))))
str(dd.xy.scaler)
base::rownames(dd.xy.scaler)
rm(XY.train.all)
XY.test = as.data.frame(do.call(rbind, apply(ori.XY.test, 1, function(x){(x-dd.xy.scaler['sp.mean',])/dd.xy.scaler['sp.sd',]} ) ))
str(XY.test)
# ... view data ...
par(mfrow=c(2,2))
hist(ori.env.train[,2],xlim=c(1000,5500), breaks = 10)
hist(ori.env.test[,2],xlim=c(1000,5500), breaks = 10)
hist(scale(ori.env.test[,2]))
hist(scale.env.test[,2])
par(mfrow=c(1,2))
hist(XY.test[,2])
hist(XY.train[,2])
rm(dd.env.scaler, dd.xy.scaler)
s.otu.train = as.matrix(otu.train)
attr(s.otu.train, 'dimnames') = NULL
str(s.otu.train)
# set variables
formula.env = 'envDNN'
lambda.env = .1
alpha.env = .5
lambda.sp = .1
alpha.sp = .9
hidden = c(50L,50L,10L)
acti.sp = 'relu'
drop = .3
save(s.otu.train,scale.env.train, XY.train,  s.otu.test, scale.env.test, XY.test, file = "Hmsc_CD/oregon_ada/data/yuanghen_mod_data.rdata")
s.otu.test = as.matrix(otu.test)
attr(s.otu.test, 'dimnames') = NULL
str(s.otu.test)
save(s.otu.train,scale.env.train, XY.train,  s.otu.test, scale.env.test, XY.test, file = "Hmsc_CD/oregon_ada/data/yuanghen_mod_data.rdata")
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
load("data/yuanghen_mod_data.rdata")
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
source("code_sjSDM/S1_read_data.r")
raretaxa <- which(colSums(Y.train.pa > 0) < 10)
length(raretaxa)
Y.train.pa_min10 <- as.matrix(Y.train.pa[, -raretaxa]) # reduced species
rm(raretaxa)
XFormula1 <- as.formula(~be10+B11_median+mean.EVI+insideHJA + Ess + ht + ht.r500 + cov4_16 + cov4_16.r500 + mTopo)
# check names
all(all.vars(XFormula1) %in% names(X.train))
str(X.train)
raretaxa <- which(colSums(Y.train.pa > 0) < 10)
length(raretaxa)
Y.train.pa_min10 <- as.matrix(Y.train.pa[, -raretaxa]) # reduced species
rm(raretaxa)
XFormula1 <- as.formula(~be10+B11_median+mean.EVI+insideHJA + Ess + ht + ht.r500 + cov4_16 + cov4_16.r500 + mTopo)
# check names
all(all.vars(XFormula1) %in% names(X.train))
str(X.train)
xy.scale <- X.train[,c("UTM_N", "UTM_E")]
str(X.train)
head(S.train)
xy.scale <- scale(S.train[,c("UTM_E", "UTM_N")])
head(xy.scale)
seq(0,0.1,0.001)
seq(0, 1, 0.1)
seq(0, 1, 0.2)
seq(0, 0.1, 0.001)
seq(0, 0.1, 0.01)
load("Hmsc_CD/oregon_ada/results_sjSDM/oregon_trial.rdata")
getwd()
load("results_sjSDM/oregon_trial.rdata")
getwd()
load("results_sjSDM/oregon_trial.rdata")
load("Hmsc_CD/oregon_ada/results_sjSDM/oregon_trial.rdata")
head(link_pred)
summary(link_pred)
family(link="probit")
binomial(link="probit")
binomial(link="probit")$linkinv
n <- 1000
beta0 <- -1.6
beta1 <- 0.03
x <- runif(n=n, min=18, max=60)
pi_x <- exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x))
y <- rbinom(n=length(x), size=1, prob=pi_x)
data <- data.frame(x, pi_x, y)
names(data) <- c("age", "pi", "y")
print(data)
glm2 <- glm(y ~ x, family = binomial)
summary(glm2)
pred_link_scale <- predict(glm2, type = "link")
n <- 100
beta0 <- -1.6
beta1 <- 0.03
x <- runif(n=n, min=18, max=60)
pi_x <- exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x))
y <- rbinom(n=length(x), size=1, prob=pi_x)
data <- data.frame(x, pi_x, y)
names(data) <- c("age", "pi", "y")
print(data)
head(data)
glm2 <- glm(y ~ x, family = binomial)
summary(glm2)
pred_link_scale <- predict(glm2, type = "link")
head(pred_link_scale)
pred_resp_scale <- predict(glm2, type = "response")
head(pred_resp_scale)
binomial(link="probit")$linkinv
binomial(link="probit")$linkinv(pred_link_scale)
pred_transformed <- binomial(link="probit")$linkinv(pred_link_scale)
pred_transformed == pred_resp_scale
binomial(link="probit")$linkinv
glm2 <- glm(y ~ x, family = binomial(link = "probit"))
summary(glm2)
pred_link_scale <- predict(glm2, type = "link")
head(pred_link_scale)
pred_resp_scale <- predict(glm2, type = "response")
head(pred_resp_scale)
pred_transformed <- binomial(link="probit")$linkinv(pred_link_scale)
head(pred_transformed)
pred_transformed == pred_resp_scale
all(pred_transformed == pred_resp_scale)
head(link_pred)
rm(b, beta0, beta1, x, pi_x, y, data, glm2, pred_link_scale, pred_resp_scale, pred_transformed)
rm(n, b, beta0, beta1, x, pi_x, y, data, glm2, pred_link_scale, pred_resp_scale, pred_transformed)
head(link_pred)
summary(link_pred)
pred_resp <- binomial(link="probit")$linkinv(link_pred)
head(link_pred)
link_pred[1:10,1:10]
(binomial(link = "probit")$linkinv(raw_pred))[1:10,1:10]
summary(raw_pred)
(binomial(link = "probit")$linkinv(raw_pred))[1:10,1:10]
link_pred[1:10,1:10]
(binomial(link = "logit")$linkinv(raw_pred))[1:10,1:10]
link_pred[1:10,1:10]
(binomial(link = "probit")$linkinv(raw_pred))[1:10,1:10]
source("code_sjSDM/S1_read_data.r")
here::here()
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
setwd(wd)
wd <- here::here()
setwd(wd)
setwd("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada")
source("code_sjSDM/S1_read_data.r")
rm(wd)
raretaxa <- which(colSums(Y.train.pa > 0) < 10)
length(raretaxa)
Y.train.pa_min10 <- as.matrix(Y.train.pa[, -raretaxa]) # reduced species
rm(raretaxa)
m1 <- matrix(c(1,5,3,7,3,4,5,5,7,8,8,10), ncol = 3)
m1
m2 <- matrix(c(2,0,3,0,3,4,10,5,10,5,7,1), ncol = 3)
m1 * m2
m1
m2
m1 * m2
m1 <- matrix(c(1,5,3,7,3,4,5,5,7,8,8,10), ncol = 3)
m2 <- matrix(c(2,0,3,0,3,4,10,5,10,5,7,10), ncol = 3)
m1
m2
m1 * m2
prob.cm <- function(pred_data, obs_data){
TP <- sum(pred_data * obs_data)
FP <- sum(pred_data * (1 - obs_data))
TN <- sum((1 - pred_data) * (1 - obs_data))
FN <- sum((1 - pred_data) * obs_data)
print(matrix(TP, FP, TN, FN), ncol = 2)
return(list(TP, FP, TN, FN))
}
cm <- prob.cm(link_pred, Y.train.pa_min10)
m1 * m2
sum(m1 * m2)
colSums(m1 * m2)
prob.cm <- function(pred_data, obs_data){
TP <- colSums(pred_data * obs_data)
FP <- colSums(pred_data * (1 - obs_data))
TN <- colSums((1 - pred_data) * (1 - obs_data))
FN <- colSums((1 - pred_data) * obs_data)
return(list(TP, FP, TN, FN))
}
cm <- prob.cm(link_pred, Y.train.pa_min10)
prob.cm <- function(pred_data, obs_data){
TP <- colSums(pred_data * obs_data)
FP <- colSums(pred_data * (1 - obs_data))
TN <- colSums((1 - pred_data) * (1 - obs_data))
FN <- colSums((1 - pred_data) * obs_data)
return(cbind(TP=TP, FP=FP, TN=TN, FN=FN))
}
cm <- prob.cm(link_pred, Y.train.pa_min10)
haed(cm)
head(cm)
cm.bin <- prob.cm(link_pred>0, Y.train.pa_min10)
head(cm.bin)
colSums(link_pred>0)
load("J:/UEA/gitHRepos/HJA_analyses_Kelpie/Hmsc_CD/oregon_ada/results_sjSDM/oregon_trial_tune.rdata")
str(model, max.level = 1)
str(tune_results, max.level = 1)
tune_results$short_summary
head(tune_results$short_summary)
str(tune_results, max.level = 1)
str(tune_results$tune_results, max.level = 1)
str(tune_results$tune_results[[1]], max.level = 1)
str(tune_results$tune_results[[1]][[1]], max.level = 1)
str(tune_results, max.level = 1)
haed(tune_results$summary)
head(tune_results$summary)
str(tune_results, max.level = 1)
summary(tune_results$summary
)
60*5
head(tune_results$summary) # here 60 iterations, times 5 for cross validation = 60*5
str(tune_results, max.level = 1)
tune_results$settings
str(tune_results, max.level = 1)
str(tune_results$tune_results[[1]][[1]], max.level = 1) # cross validation results
str(tune_results, max.level = 1)
str(tune_results$tune_results[[1]], max.level = 1) # cross validation results
str(tune_results$tune_results[[1]][[1]], max.level = 1) # cross validation results
max(tune_results$short_summary$AUC_test)
summary(tune_results)
best
